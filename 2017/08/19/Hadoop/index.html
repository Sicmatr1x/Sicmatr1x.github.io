<!DOCTYPE html>


<html lang="en">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Hadoop |  
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  <link rel="stylesheet" href="/dist/main.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">
  <link rel="stylesheet" href="/css/custom.css">
  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@latest/build/styles/vs2015.min.css"><!- highlight.js ->
  <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@latest/build/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
</head>

</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Hadoop"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Hadoop
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/2017/08/19/Hadoop/" class="article-date">
  <time datetime="2017-08-19T09:38:04.000Z" itemprop="datePublished">2017-08-19</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">8.1k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">38 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><h3 id="Hadoop："><a href="#Hadoop：" class="headerlink" title="Hadoop："></a>Hadoop：</h3><p>Hadoop是一个框架，它是由Java语言来实现的。Hadoop是处理大数据技术.Hadoop可以处理云计算产生大数据。</p>
<p>CDH商业版：</p>
<p>Cloudera CDH是Hadoop的一个版本，比Apache Hadoop的优点如下：</p>
<ol>
<li>CDH基于稳定版Apache Hadoop，并应用了最新Bug修复或者Feature的Patch。Cloudera常年坚持季度发行Update版本，年度发行Release版本，更新速度比Apache官方快，而且在实际使用过程中CDH表现无比稳定，并没有引入新的问题。</li>
<li>Cloudera官方网站上安装、升级文档详细，省去Google时间。</li>
<li>CDH支持Yum/Apt包，Tar包，RPM包，Cloudera Manager四种方式安装，总有一款适合您。官方网站推荐Yum/Apt方式安装，其好处如下：<ul>
<li>联网安装、升级，非常方便。当然你也可以下载rpm包到本地，使用Local Yum方式安装。</li>
<li>自动下载依赖软件包，比如要安装Hive，则会级联下载、安装Hadoop。</li>
<li>Hadoop生态系统包自动匹配，不需要你寻找与当前Hadoop匹配的Hbase，Flume，Hive等软件，Yum/Apt会根据当前安装Hadoop版本自动寻找匹配版本的软件包，并保证兼容性。</li>
<li>自动创建相关目录并软链到合适的地方（如conf和logs等目录）；自动创建hdfs, mapred用户，hdfs用户是HDFS的最高权限用户，mapred用户则负责mapreduce执行过程中相关目录的权限。</li>
</ul>
</li>
</ol>
<h3 id="大数据的4个V"><a href="#大数据的4个V" class="headerlink" title="大数据的4个V:"></a>大数据的4个V:</h3><ol>
<li>Velocity：实现快速的数据流传</li>
<li>Variety： 具有多样的数据类型</li>
<li>Volume： 存有海量的数据规模（TB，PB，EB级别）</li>
<li>Value：存在着巨大的价值</li>
</ol>
<hr>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><img src="Hadoop项目结构图.jpg" />

<p>Hadoop实际上就是谷歌三宝的开源实现，</p>
<p>Hadoop MapReduce对应Google MapReduce，</p>
<p>HBase对应BigTable，</p>
<p>HDFS对应GFS。HDFS（或GFS）为上层提供高效的非结构化存储服务，</p>
<p>HBase（或BigTable）是提供结构化数据服务的分布式数据库，Hadoop MapReduce（或Google MapReduce）是一种并行计算的编程模型，用于作业调度。</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。</p>
<h3 id="HDFS-Hadoop-Distributed-File-System-："><a href="#HDFS-Hadoop-Distributed-File-System-：" class="headerlink" title="HDFS(Hadoop Distributed File System)："></a>HDFS(Hadoop Distributed File System)：</h3><ul>
<li>默认的最基本的存储单位是64M的数据块。</li>
<li>和普通文件系统相同的是，HDFS中的文件是被分成64M一块的数据块存储的。</li>
<li>不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。</li>
</ul>
<h3 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h3><p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。</p>
<h3 id="联机事务处理OLTP-On-line-Transaction-Processing-、联机分析处理OLAP-On-Line-Analytical-Processing"><a href="#联机事务处理OLTP-On-line-Transaction-Processing-、联机分析处理OLAP-On-Line-Analytical-Processing" class="headerlink" title="联机事务处理OLTP(On-line Transaction Processing)、联机分析处理OLAP(On-Line Analytical Processing)"></a>联机事务处理OLTP(On-line Transaction Processing)、联机分析处理OLAP(On-Line Analytical Processing)</h3><p>OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 </p>
<img src="OLTP与OLAP之间的比较.jpg" />

<p>分析型数据不允许update、delete操作</p>
<h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><p>Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
<h3 id="ZooKepper"><a href="#ZooKepper" class="headerlink" title="ZooKepper"></a>ZooKepper</h3><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p>
<p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。<br>ZooKeeper包含一个简单的原语集，提供Java和C的接口。</p>
<h3 id="Mahout"><a href="#Mahout" class="headerlink" title="Mahout"></a>Mahout</h3><img src="Mahout.PNG" />

<p>Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。</p>
<h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><h3 id="支持平台"><a href="#支持平台" class="headerlink" title="支持平台"></a>支持平台</h3><ul>
<li>GNU/Linux是产品开发和运行的平台。Hadoop已在有2000个节点的GNU/Linux主机组成的集群系统上得到验证。</li>
<li>Win32平台是作为<code>开发平台</code>支持的。由于分布式操作尚未在Win32平台上充分测试，所以还不作为一个<code>生产平台</code>被支持。</li>
</ul>
<h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><h4 id="安装-VMware"><a href="#安装-VMware" class="headerlink" title="安装 VMware"></a>安装 VMware</h4><h4 id="安装-Ubuntu"><a href="#安装-Ubuntu" class="headerlink" title="安装 Ubuntu"></a>安装 Ubuntu</h4><h4 id="安装-jdk"><a href="#安装-jdk" class="headerlink" title="安装 jdk"></a>安装 jdk</h4><p>解压<code>tar -vzfx jdk-1.7.0.tar.gz</code></p>
<p>配环境变量<code>sudo vim /etc/profile</code></p>
<pre><code class="bash">export JAVA_HOME=/home/master0/Desktop/jkd1.7.0_80
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH</code></pre>
<p>使配置文件生效：<code>source /etc/profile</code></p>
<h4 id="安装Hadoop-1"><a href="#安装Hadoop-1" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><p>配环境变量<code>sudo vim /etc/profile</code></p>
<pre><code class="bash">export JAVA_HOME=/home/master0/Desktop/jdk1.7.0_80
export HADOOP_HOME=/home/master0/Desktop/hadoop-2.6.0
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.6.0.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:$HADOOP_HOME/share/hadoop/common/lib/commons-1.2.jar:$CLASSPATH
export PATH=$JAVA_HOME/bin:$PATH
export PATH=$HADOOP_HOME/bin:$PATH</code></pre>
<p>配置<code>~/hadoop-2.6.0/etc/hadoop/hadoop-env.sh</code></p>
<pre><code class="bash"># The java implementation to use.
export JAVA_HOME=/home/master0/Desktop/jdk1.7.0_80</code></pre>
<h4 id="测试hadoop"><a href="#测试hadoop" class="headerlink" title="测试hadoop"></a>测试hadoop</h4><pre><code class="bash">hadoop version</code></pre>
<hr>
<h4 id="使用hadoop的本地单独模式"><a href="#使用hadoop的本地单独模式" class="headerlink" title="使用hadoop的本地单独模式"></a>使用hadoop的本地单独模式</h4><p>对某目录下的文档进行单词数的统计</p>
<pre><code class="bash">$ cd  /home/hadoop/    
$ mkdir  input
$ cp   $HADOOP_HOME/etc/hadoop/*.xml   input/
$ hadoop  jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output &#39;dfs[a-z.]+&#39;
$ cat output/*</code></pre>
<h4 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h4><p>修改主机名</p>
<pre><code class="bash">sudo gedit /etc/hostname</code></pre>
<hr>
<h4 id="配置静态IP"><a href="#配置静态IP" class="headerlink" title="配置静态IP"></a>配置静态IP</h4><pre><code class="bash">sudo gedit /etc/network/interfaces</code></pre>
<p>编辑-&gt;虚拟网络编辑器-&gt;查看NAT模式的子网地址</p>
<p>例如为：231</p>
<p>master</p>
<pre><code class="bash">auto eth0
iface eth0 inet static
address 192.168.231.129
netmask 255.255.255.0
network 192.168.231.0
boardcast 192.168.231.255
gateway 192.168.231.2
dns-nameservers 192.168.1.1 8.8.8.8 8.8.8.4

ping 192.168.231.130</code></pre>
<p>serve1</p>
<pre><code class="bash">auto eth0
iface eth0 inet static
address 192.168.231.130
netmask 255.255.255.0
network 192.168.231.0
boardcast 192.168.231.255
gateway 192.168.231.2
dns-nameservers 192.168.1.1 8.8.8.8 8.8.8.4

ping 192.168.231.129</code></pre>
<p>若访问不了网页的话可以将物理机的dns填写在dns-nameservers第一个</p>
<p>若拖文件拖不进虚拟机需检查：</p>
<p>虚拟机ping与其对应的模式的虚拟网卡可不可以ping通</p>
<p>主机ping与虚拟机可不可以ping通</p>
<p>VMware Network Adapter VMnet1:桥接模式虚拟网卡</p>
<p>VMware Network Adapter VMnet8:NAT模式虚拟网卡</p>
<h4 id="修改hosts文件"><a href="#修改hosts文件" class="headerlink" title="修改hosts文件"></a>修改hosts文件</h4><pre><code class="bash">sudo gedit /etc/hosts

192.168.231.129  master
192.168.231.130  serve1
192.168.231.131  serve2</code></pre>
<h4 id="安装ssh"><a href="#安装ssh" class="headerlink" title="安装ssh"></a>安装ssh</h4><pre><code class="bash">sudo apt-get install ssh</code></pre>
<p>安装完毕就会出现<code>/home/master/.ssh</code>文件夹</p>
<p>然后需要生成了一个公钥</p>
<pre><code class="bash">ssh-keygen -t rsa -P &#39;&#39;

会生成
id_rsa  id_rsa.pub  known_hosts</code></pre>
<p>id_rsa为私钥</p>
<pre><code class="bash">-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEAp6JLYxM9lm/ciNG5SuAd/0WBBY0VN98w1KLad5GrkZhM5iZ1
mKnl1JHhT14//QSqtJ/tAAo8P1EZspvziS1q77DVBF4L/kInl0KEZOiFWMUOKqDj
y+TWLSZmBK9uP5J2cb2wnIMZ4HeWw0y8hnaCpfg4FNVm8WL/EQh++EHx4VBQv0bt
4s3qZ9LgYM0MGDrizYKCZ92vRE2CVgLlpAzXvD2uFfxlFwJl02l35fjaIW2ed6PV
HrnT8D6BrpUIdKWzWsevj3W6IfO0upBtqOygJw0RxYSx646nDDdFXIk7bzdVFXdr
sOjOblsPGqTJs+aApEQB3avOUZI0EixCr2h7/wIDAQABAoIBAQCY1je1lQ1J46NG
ezBdPAkdfNktnnwB/NQginp1GbM7g4hZLid5kS2iqX6rRltA7MhW9pi2uJ5FfEPZ
vKZGI8qjzq3o1XZJ0zcVief7uKQbU06fPyFx/KnpcGEDVI9IFtk2yqQDjuRA68fh
OE2KqvJjL/Sxyf+ZhZDYjs50ums16PHxXlhAaP8EI78Dcff5sx+ZoKTVGum4Jrdl
h0cXeDBcxJZg7wEtHPEUrduaiwEv88fD7aw2QwsYdCuPECncltR57iPi95hr7uaW
XdtRZ+mAey5sBxJmZKrlPE6kK3yAvSs1tP0yz4R4azAYQqTpLmxcfMrqWRwb3IMA
9Rl98FIBAoGBANpNaYJgaTvDT2Nski6sTu1oPefow4tosvPE1jZ/gWXExJ9m1OiI
GcZGG0nM+UCx85+//B6gyLdvmUGgxN9vzmY3myuhQ9iesep7W+DiqDnz2J/VRM98
eEso6P1jevlC90WJh1wNrVIuzxuN/5A5LghjNNuHCnZzJTRuSKjISjRhAoGBAMSU
9hdNDliOXDIIRs/vjwiRuLvbECMFqETSyFdnc91dAi2cYfwlfKFlWGSPFO/LuTvL
9PfWaKgfuAzMiZ5JoMPlo5iX8atX1V4Naz7e3OBR9rhyD0oO4aNyKtvDv7tIWTxm
eWw/4hlmPp/wGYgfxlPOrbVfJcESYk9FmRxxeoxfAoGAP87ozCcKG2HXTqRphiLv
Xw1dKvAqWBFeXUpnor5aQDjnkAAqs100y3OqfkPfhz18jHE9bGZqxNNl5HztjrHL
jq0qOfKFNkgMkRFFpdIagfX4l59q4YrsTmvCzm3JgBpG1JiCbDHDO4ZbGx7CWJGe
Fu2IgbJTKJQ3h7/ElTEWH4ECgYEAoxOr/vJ2hzI5+2twSwlBT+uLI5P8FAGacNWn
SxLQRH/m0a2cf48dj8pCBNHJnZAUby2oX30nvujpRvza4UvVKQ20pF7QJcMshuR8
5l/9Pb3g/WvpkRc9SdjpAvylbpj7JicgbZOlXkq6gvWsSIeLgHTBF+gBquQ0V+y1
sqnU7uMCgYBMSR2QDG5TuSp7pNOFBFuqhOCrUHZmKqoHCZ7rSh3etxc8D5tLXciE
APNWfGqSE2aT/PJgqNoxl5p42bnZrv3cXJuiD9Wid6yFzDH0oUa9K66vy1SWV+B8
3rHha5wLzizgNUQZjh1XSndp1WekYCLjV+Bn8b/odBClcHKX7M/BOg==
-----END RSA PRIVATE KEY-----</code></pre>
<p>id_rsa.pub为公钥</p>
<pre><code class="bash">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnoktjEz2Wb9yI0blK4B3/RYEFjRU33zDUotp3kauRmEzmJnWYqeXUkeFPXj/9BKq0n+0ACjw/URmym/OJLWrvsNUEXgv+QieXQoRk6IVYxQ4qoOPL5NYtJmYEr24/knZxvbCcgxngd5bDTLyGdoKl+DgU1WbxYv8RCH74QfHhUFC/Ru3izepn0uBgzQwYOuLNgoJn3a9ETYJWAuWkDNe8Pa4V/GUXAmXTaXfl+NohbZ53o9UeudPwPoGulQh0pbNax6+Pdboh87S6kG2o7KAnDRHFhLHrjqcMN0VciTtvN1UVd2uw6M5uWw8apMmz5oCkRAHdq85RkjQSLEKvaHv/ master@ubuntu</code></pre>
<p>要想免密登录则需要被免密登录方的公钥：这里可以先将各台分机的公钥发送给主机master，然后再由master合成一个文件再发送给分机。这样每台机器都会有其它所有机器的公钥</p>
<p>生成公钥文件</p>
<pre><code class="bash">cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys</code></pre>
<pre><code class="bash">authorized_keys如下，其实和公钥相同：
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnoktjEz2Wb9yI0blK4B3/RYEFjRU33zDUotp3kauRmEzmJnWYqeXUkeFPXj/9BKq0n+0ACjw/URmym/OJLWrvsNUEXgv+QieXQoRk6IVYxQ4qoOPL5NYtJmYEr24/knZxvbCcgxngd5bDTLyGdoKl+DgU1WbxYv8RCH74QfHhUFC/Ru3izepn0uBgzQwYOuLNgoJn3a9ETYJWAuWkDNe8Pa4V/GUXAmXTaXfl+NohbZ53o9UeudPwPoGulQh0pbNax6+Pdboh87S6kG2o7KAnDRHFhLHrjqcMN0VciTtvN1UVd2uw6M5uWw8apMmz5oCkRAHdq85RkjQSLEKvaHv/ master@ubuntu</code></pre>
<pre><code class="bash">分机serve1复制公钥到master主机上：
scp .ssh/id_rsa.pub master@master:/home/master/id_rsa_1.pub
将分机serve1的公钥追加到主机的authorized_keys上
cat id_rsa_1.pub &gt;&gt; .ssh/authorized_keys</code></pre>
<p>重复以上两步直到主机master的authorized_keys有所有分机的公钥，再进行分发操作</p>
<pre><code class="bash">scp .ssh/authorized_keys master@serve1:/home/master/.ssh/authorized_keys
scp .ssh/authorized_keys master@serve2:/home/master/.ssh/authorized_keys</code></pre>
<p>分发完毕后即可进行测试：</p>
<pre><code class="bash">ssh master
ssh serve1
能连接成功即可</code></pre>
<p>SSH免密码设置失败解决</p>
<ol>
<li>权限问题</li>
</ol>
<p>.ssh目录，以及/home/当前用户 需要700权限，参考以下操作调整</p>
<pre><code class="bash">$sudo   chmod   777   ~/.ssh

$sudo  chmod 700  /home/当前用户</code></pre>
<p>.ssh目录下的authorized_keys文件需要600或644权限，参考以下操作调整</p>
<pre><code class="bash">$sudo chmod   644   ~/.ssh/authorized_keys</code></pre>
<ol start="2">
<li>StrictModes问题</li>
</ol>
<pre><code class="bash">$sudo gedit /etc/ssh/sshd_config</code></pre>
<p>找到</p>
<pre><code class="bash">\#StrictModes yes</code></pre>
<p>改成</p>
<pre><code class="bash">StrictModes no</code></pre>
<p>如果还不行，可以用<code>ssh -vvv 目标机器ip</code> 查看详情</p>
<h4 id="配置Hadoop集群"><a href="#配置Hadoop集群" class="headerlink" title="配置Hadoop集群"></a>配置Hadoop集群</h4><p>以下将会修改多个Hadoop配置文件均位于<code>hadoop-2.6.0/etc</code>目录下</p>
<p>修改：<code>hadoop-env.sh</code> 、<code>yarn-env.sh</code></p>
<pre><code class="bash">gedit etc/hadoop/hadoop-env.sh

# The java implementation to use.
export JAVA_HOME=/home/master/jdk1.7.0_80</code></pre>
<pre><code class="bash">gedit etc/hadoop/yarn-env.sh</code></pre>
<p>core-site.xml</p>
<p>core-site.xml的完整参数请参考: <a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml</a></p>
<pre><code class="bash">gedit etc/hadoop/core-site.xml</code></pre>
<p><code>/home/hadoop/tmp</code> 目录如不存在，则先mkdir手动创建</p>
<pre><code class="xml">&lt;configuration&gt;

 &lt;property&gt;
   &lt;name&gt;fs.defaultFS&lt;/name&gt;
   &lt;value&gt;hdfs://master:9000&lt;/value&gt;&lt;!--主机名:端口号--&gt;     
 &lt;/property&gt;
 &lt;property&gt;
     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
     &lt;value&gt;/home/master/tmp&lt;/value&gt;&lt;!--/tmp/hadoop-${user.name}--&gt;   
 &lt;/property&gt; 

&lt;/configuration&gt;</code></pre>
<p>hdfs-site.xml</p>
<p>hdfs-site.xml的完整参数请参考: <a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p>
<pre><code class="bash">gedit etc/hadoop/hdfs-site.xml</code></pre>
<pre><code class="xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.ipc.address&lt;/name&gt;
        &lt;value&gt;0.0.0.0:50020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;
        &lt;value&gt;0.0.0.0:50075&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/home/master/data/namenode&lt;/value&gt;
        &lt;!--元数据--&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:/home/master/data/datanode&lt;/value&gt;
        &lt;!--数据块--&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;slave1:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
        &lt;!--备份数量--&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.permissions&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
        &lt;!--权限验证--&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
<p>配置slaves分机列表</p>
<pre><code class="bash">gedit etc/hadoop/slaves</code></pre>
<pre><code class="bash">master
serve1</code></pre>
<p>分发配置文件到集群的其它机器</p>
<pre><code class="bash">scp -r hadoop-2.6.0/etc/hadoop/ master@serve1:/home/master/hadoop-2.6.0/etc/</code></pre>
<p>格式化hdfs</p>
<pre><code class="bash">hdfs namenode -format</code></pre>
<p>等看到执行信息有has been successfully formatted表示格式化ok</p>
<p>启动 dfs</p>
<pre><code class="bash">hadoop-2.6.0/sbin/start-dfs.sh</code></pre>
<p>验证hadoop是否启动成功：</p>
<pre><code class="bash">$jps
显示有：
4895 DataNode
4775 NameNode</code></pre>
<h4 id="安装-MapReduce"><a href="#安装-MapReduce" class="headerlink" title="安装 MapReduce"></a>安装 MapReduce</h4><p>mapred-site.xml的完整参数请参考<a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></p>
<p>将mapred-site.xml.template改名成mapred-site.xml</p>
<pre><code class="xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;master:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
        &lt;value&gt;master:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
<p>yarn-site.xml</p>
<p>yarn-site.xml的完整参数请参考: <a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></p>
<pre><code class="xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
        &lt;value&gt;master:8030&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
        &lt;value&gt;master:8025&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
        &lt;value&gt;master:8040&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
<h4 id="启动yarn"><a href="#启动yarn" class="headerlink" title="启动yarn"></a>启动yarn</h4><pre><code class="bash">hadoop-2.6.0/sbin/start-yarn.sh</code></pre>
<pre><code class="bash">$jps
多了ResourceManager和NodeManager表示启动yarn成功
SecondaryNameNode
ResourceManager
NameNode</code></pre>
<pre><code class="bash">hadoop-2.6.0/sbin/start-dfs.sh
hadoop-2.6.0/sbin/start-yarn.sh

jps
master节点上有几下3个进程：
7482 ResourceManager
7335 SecondaryNameNode
7159 NameNode
slave1、slave2上有几下2个进程：
2296 DataNode
2398 NodeManager

hadoop-2.6.0/sbin/stop-dfs.sh
hadoop-2.6.0/sbin/stop-yarn.sh</code></pre>
<p>或打开浏览器访问：hdfs管理界面: <a href="http://master:50070" target="_blank" rel="noopener">http://master:50070</a></p>
<p>yarn的管理界面: <a href="http://master:8088/" target="_blank" rel="noopener">http://master:8088/</a></p>
<p>查看hadoop状态</p>
<pre><code class="bash">hdfs dfsadmin -report //查看hdfs的状态报告

yarn  node -list   //查看yarn的基本信息

Secondary// TODO
NameNode 元数据
DataNode 数据块</code></pre>
<hr>
<h2 id="HDFS文件系统"><a href="#HDFS文件系统" class="headerlink" title="HDFS文件系统"></a>HDFS文件系统</h2><p>hadoop实现了一个分布式文件系统HDFS(Hadoop Distributed File System)</p>
<img src="HDFS架构.png" />

<p>元数据：用于描述数据的数据。</p>
<p>NameNode 主服务器，用来管理整个文件系统的命名空间和元数据，以及处理来自外界的文件访问请求。整个集群中只有一个。含有：</p>
<ol>
<li>命名空间：整个分布式文件系统的目录结构</li>
<li>数据块与文件名的映射表</li>
<li>每个数据块副本的位置信息(每个数据块默认3个副本)</li>
</ol>
<p>元数据保存在NameNode的内存当中(1G内存可存放1000000个块对应的元数据信息，缺省每块64M计算可对应64T实际数据)</p>
<p>DataNode通过心跳包(Heartbeats)与NameNode通讯</p>
<p>HA(High Available)高可用</p>
<p>DataNode 用来实际存储和管理文件的数据块</p>
<p>数据块-64M(128M)数据块+备份公用一个ID</p>
<p>主从架构：1个NameNode对应n个DataNode</p>
<pre><code class="bash">
client-java app -&gt; data NameNode(客户端向NameNode发起请求)
client-sid datanode-&gt; datanode -&gt; r/w -&gt; dfs file(NameNode返回对应的DataNode给客户端让客户端来通过DataNode进行访问)
                   -&gt; namenode(向NameNode汇报情况)</code></pre>
<h3 id="JVM从HDFS读取文件流程"><a href="#JVM从HDFS读取文件流程" class="headerlink" title="JVM从HDFS读取文件流程"></a>JVM从HDFS读取文件流程</h3><img src="HDFS数据的读取过程.png" />

<p>client会从距离最近的机子上读取</p>
<h4 id="HDFS文件存储的组织与读写："><a href="#HDFS文件存储的组织与读写：" class="headerlink" title="HDFS文件存储的组织与读写："></a>HDFS文件存储的组织与读写：</h4><p>数据写入</p>
<ol>
<li>客户端调用FileSystem 实例的create 方法，创建文件。NameNode 通过一些检查，比如文件是否存在，客户端是否拥有创建权限等;通过检查之后，在NameNode 添加文件信息。注意，因为此时文件没有数据，所以NameNode 上也没有文件数据块的信息。</li>
<li>创建结束之后， HDFS 会返回一个输出流DFSDataOutputStream 给客户端。</li>
<li>客户端调用输出流DFSDataOutputStream 的write 方法向HDFS 中对应的文件写入数据。</li>
<li>数据首先会被分包，这些分包会写人一个输出流的内部队列Data 队列中，接收完数据分包，输出流DFSDataOutputStream 会向NameNode 申请保存文件和副本数据块的若干个DataNode ， 这若干个DataNode 会形成一个数据传输管道。DFSDataOutputStream 将数据传输给距离上最短的DataNode ，这个DataNode 接收到数据包之后会传给下一个DataNode 。数据在各DataNode之间通过管道流动，而不是全部由输出流分发，以减少传输开销。</li>
<li>因为各DataNode 位于不同机器上，数据需要通过网络发送，所以，为了保证所有DataNode 的数据都是准确的，接收到数据的DataNode 要向发送者发送确认包(ACK Packet ) 。对于某个数据块，只有当DFSDataOutputStream 收到了所有DataNode 的正确ACK. 才能确认传输结束。DFSDataOutputStream 内部专门维护了一个等待ACK 队列，这一队列保存已经进入管道传输数据、但是并未被完全确认的数据包。</li>
<li>不断执行第3 - 5 步直到数据全部写完，客户端调用close 关闭文件。</li>
<li>DFSDataInputStream 继续等待直到所有数据写人完毕并被确认，调用complete 方法通知NameNode 文件写入完成。NameNode 接收到complete 消息之后，等待相应数量的副本写入完毕后，告知客户端</li>
</ol>
<pre><code class="bash">查看文件
hadoop fs -cat /output/part-00000
查看hadoop文件系统
hadoop fs -ls /
hadoop fs -ls -R /output
hadoop fs -ls /output
创建文件夹
hadoop fs -mkdir /tmp
hadoop fs -mkdir /input
hadoop fs -mkdir /output
将文件放到hadoop文件系统-put 当前路径 /home/master/input 放到的路径
hadoop fs -put /home/master/input/* /input
hadoop fs -get /output output

hadoop fs -rm -R /input
hadoop fs -rm -r /output/output
hadoop fs -mv /output/output/part-r-00000 /output/part-r-00000</code></pre>
<p>运行例子</p>
<pre><code class="bash">hadoop jar hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep /input /output &#39;dfs[a-z.]+&#39;</code></pre>
<h3 id="Hadoop-IO"><a href="#Hadoop-IO" class="headerlink" title="Hadoop IO"></a>Hadoop IO</h3><p>HDFS数据完整性</p>
<p>校验和+后台进程</p>
<p>文件数据结构-解决大量小文件</p>
<p>SequenceFile：用流来读写</p>
<p>MapFile</p>
<h2 id="MapReduce-1"><a href="#MapReduce-1" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>Map/Reduce是一个用于大规模数据处理的分布式计算模型，它最初是由Google工程师设计并实现的，Google已经将它完整的MapReduce论文公开发布了。其中对它的定义是，Map/Reduce是一个编程模型（programming model），是一个用于处理和生成大规模数据集（processing and generating large data sets）的相关的实现。用户定义一个map函数来处理一个key/value对以生成一批中间的key/value对，再定义一个reduce函数将所有这些中间的有着相同key的values合并起来。很多现实世界中的任务都可用这个模型来表达。</p>
<img src="how-hadoop-runs-a-mapreduce-kob.jpg" />

<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><img src="hadoop-mapreduce-framework-architecture.jpg" />

<p>Mapper、Reduce</p>
<blockquote>
<p>运行于Hadoop的MapReduce应用程序最基本的组成部分包括一个Mapper和一个Reducer类，以及一个创建JobConf的执行程序，在一些应用中还可以包括一个Combiner类，它实际也是Reducer的实现。</p>
</blockquote>
<p>JobTracker、TaskTracker</p>
<blockquote>
<p>它们都是由一个master服务JobTracker和多个运行于多个节点的slaver服务TaskTracker两个类提供的服务调度的。master负责调度job的每一个子任务task运行于slave上，并监控它们，如果发现有失败的task就重新运行它，slave则负责直接执行每一个task。TaskTracker都需要运行在HDFS的DataNode上，而JobTracker则不需要，一般情况应该把JobTracker部署在单独的机器上。</p>
</blockquote>
<p>JobClient</p>
<blockquote>
<p>每一个job都会在用户端通过JobClient类将应用程序以及配置参数Configuration打包成jar文件存储在HDFS，并把路径提交到JobTracker的master服务，然后由master创建每一个Task（即MapTask和ReduceTask）将它们分发到各个TaskTracker服务中去执行。</p>
</blockquote>
<p>JobInProgress</p>
<blockquote>
<p>JobClient提交job后，JobTracker会创建一个JobInProgress来跟踪和调度这个job，并把它添加到job队列里。JobInProgress会根据提交的job jar中定义的输入数据集（已分解成FileSplit）创建对应的一批TaskInProgress用于监控和调度MapTask，同时在创建指定数目的TaskInProgress用于监控和调度ReduceTask，缺省为1个ReduceTask。</p>
</blockquote>
<p>TaskInProgress</p>
<blockquote>
<p>JobTracker启动任务时通过每一个TaskInProgress来launchTask，这时会把Task对象（即MapTask和ReduceTask）序列化写入相应的TaskTracker服务中，TaskTracker收到后会创建对应的TaskInProgress（此TaskInProgress实现非JobTracker中使用的TaskInProgress，作用类似）用于监控和调度该Task。启动具体的Task进程是通过TaskInProgress管理的TaskRunner对象来运行的。TaskRunner会自动装载job jar，并设置好环境变量后启动一个独立的java child进程来执行Task，即MapTask或者ReduceTask，但它们不一定运行在同一个TaskTracker中。</p>
</blockquote>
<p>MapTask、ReduceTask</p>
<blockquote>
<p>一个完整的job会自动依次执行Mapper、Combiner（在JobConf指定了Combiner时执行）和Reducer，其中Mapper和Combiner是由MapTask调用执行，Reducer则由ReduceTask调用，Combiner实际也是Reducer接口类的实现。Mapper会根据job jar中定义的输入数据集按&lt;key1,value1&gt;对读入，处理完成生成临时的&lt;key2,value2&gt;对，如果定义了Combiner，MapTask会在Mapper完成调用该Combiner将相同key的值做合并处理，以减少输出结果集。MapTask的任务全完成即交给ReduceTask进程调用Reducer处理，生成最终结果&lt;key3,value3&gt;对。这个过程在下一部分再详细介绍。</p>
</blockquote>
<img src="mapreduce运行机制.jpg" />

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><h4 id="单词统计案例"><a href="#单词统计案例" class="headerlink" title="单词统计案例"></a>单词统计案例</h4><pre><code>Mapper&lt;LongWritable, Text, Text, IntWritable&gt;
public void map(LongWritable k1, Text v1, Context context)
输入LongWritable k1, Text v1(LongWritable, Text)：序号,行
处理：从行中split出每个单词，并将每个单词的值设为1
输出Context context(Text, IntWritable)：单词,所有该单词的值的集合(数组)

Reducer&lt;Text, IntWritable, Text, IntWritable&gt;
public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)
输入Text key, Iterable&lt;IntWritable&gt; values(Text, IntWritable)：单词,所有该单词的值的集合(数组)
处理：使用迭代器Iterator来迭代每个单词的值的数组并将数组中的每个元素相加，和作为该单词新的值
输出Context context(Text, IntWritable)：单词,单词出现次数</code></pre><pre><code class="java">package mypro1;

import java.io.IOException;
import java.net.URI;
import java.util.Iterator;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;   

public class MyWordCount {

    static class MyMapper  extends  Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{  
        // 输入LongWritable k1, Text v1(LongWritable, Text)：序号,行
        public void map(LongWritable k1, Text v1, Context context) 
            throws java.io.IOException, java.lang.InterruptedException{
            // 处理：从行中split出每个单词，并将每个单词的值设为1
            String[]  lines= v1.toString().split(&quot;\\s+&quot;);
            for(String word: lines){
                // 输出Context context(Text, IntWritable)：单词,所有该单词的值的集合(数组)
                context.write(new Text(word), new IntWritable(1));
            }

            System.out.println(&quot;map......&quot;);
        }

    }

    static class  MyReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{
        // 输入Text key, Iterable&lt;IntWritable&gt; values(Text, IntWritable)：单词,所有该单词的值的集合(数组)
        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)
             throws java.io.IOException, java.lang.InterruptedException{
            // 处理：使用迭代器Iterator来迭代每个单词的值的数组并将数组中的每个元素相加，和作为该单词新的值
            int sum=0;
            Iterator&lt;IntWritable&gt;  it = values.iterator();
            while(it.hasNext()){
                sum+= it.next().get();
            }
            // 输出Context context(Text, IntWritable)：单词,单词出现次数
            context.write(key, new IntWritable(sum));    

            System.out.println(&quot;reduce......&quot;);
        }

    }

    // 定义输入文件
    private static String INPUT_PATH=&quot;hdfs://master:9000/input/hdfs-site.xml&quot;;
    // 定义输出结果到目录
    private static String OUTPUT_PATH=&quot;hdfs://master:9000/output/c/&quot;;

    public static void main(String[] args) throws Exception {    
        // 加载配置文件
        Configuration  conf=new Configuration();
        FileSystem  fs=FileSystem.get(new URI(OUTPUT_PATH),conf);
         // 若输出目录已存在则删除
        if(fs.exists(new Path(OUTPUT_PATH)))
                fs.delete(new Path(OUTPUT_PATH));

        // 开启一个作业
        Job job = new Job(conf,&quot;myjob&quot;);
        // 设置作业jar包
        job.setJarByClass(MyWordCount.class);
        // 设置作业Mapper类
        job.setMapperClass(MyMapper.class);
        // 设置作业Reducer类
        job.setReducerClass(MyReduce.class);

        // Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;定义Mapper泛型输出类

        // Reducer&lt;Text, IntWritable, Text, IntWritable&gt;定义Reducer泛型输出类，因输入与输出相同可省略
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 使用文件读取系统读取文件到作业
        FileInputFormat.addInputPath(job,new Path(INPUT_PATH));
        // 使用文件读取系统输出作业结果
        FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH));

        job.waitForCompletion(true);

    }

}</code></pre>
<h4 id="排序案例"><a href="#排序案例" class="headerlink" title="排序案例"></a>排序案例</h4><pre><code>7 5
2 1
2 2
9 3
1 8
4 5
6 2
0 7

Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;
public void map(LongWritable k1, Text v1, Context context)
输入LongWritable k1, Text v1(LongWritable, Text)：序号,行
处理
输出Context context(MyK2, LongWritable)：两个数,后面那个数(与排序无关,为空都可以)

Reducer&lt;MyK2, LongWritable,LongWritable, LongWritable&gt;
public void reduce(MyK2 myk2, Iterable&lt;LongWritable&gt; v2s,Context context)
输入MyK2 myk2, Iterable&lt;LongWritable&gt; v2s(MyK2, LongWritable)：两个数，后面那个数(与排序无关,为空都可以)
处理
输出Context context(LongWritable, LongWritable)：第一个数,第二个数

0    7
1    8
2    1
2    2
4    5
6    2
7    5
9    3</code></pre><pre><code class="java">package demo;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.net.URI;
import java.util.Iterator;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;   

public class Sort {

    static class MyK2 implements WritableComparable&lt;MyK2&gt;{
         public long myk2;  
         public long myv2;  
         public MyK2() {  
                // TODO Auto-generated constructor stub  
         }  

         public MyK2(long myk2, long myv2) {  
             this.myk2 = myk2;  
             this.myv2 = myv2;  
         }

        @Override
        public void readFields(DataInput in) throws IOException {
            this.myk2=in.readLong();  
            this.myv2=in.readLong();
        }

        @Override
        public void write(DataOutput out) throws IOException {
            out.writeLong(myk2);  
            out.writeLong(myv2);
        }

        @Override
        public int compareTo(MyK2 my) {
            long temp=this.myk2-my.myk2; 
            if(temp!=0){
                return (int) temp; 
            }
            return (int) (this.myv2-my.myv2);
        }  
    }

    static class MyMapper  extends  Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;{  
         public void map(LongWritable k1, Text v1, Context context) 
                         throws java.io.IOException, java.lang.InterruptedException
         {
            String[]  lines= v1.toString().split(&quot;\\s&quot;);
            MyK2 myK2 = new MyK2(Long.parseLong(lines[0]), Long.parseLong(lines[1]));
            context.write(myK2, new LongWritable(Long.parseLong(lines[0])));
            System.out.println(&quot;map......&quot;);
         }

    }

    static class  MyReduce extends Reducer&lt;MyK2, LongWritable,LongWritable, LongWritable&gt;{
         public void reduce(MyK2 myk2, Iterable&lt;LongWritable&gt; v2s,Context context) throws java.io.IOException, java.lang.InterruptedException
         {
             context.write(new LongWritable(myk2.myk2), new LongWritable(myk2.myv2));    
             System.out.println(&quot;reduce......&quot;);
         }

    }

    private static String INPUT_PATH=&quot;hdfs://master:9000/input/num&quot;;
    private static String OUTPUT_PATH=&quot;hdfs://master:9000/output/num/&quot;;

    public static void main(String[] args) throws Exception {    

        Configuration  conf=new Configuration();
        FileSystem  fs=FileSystem.get(new URI(OUTPUT_PATH),conf);

        if(fs.exists(new Path(OUTPUT_PATH)))
                fs.delete(new Path(OUTPUT_PATH));

        Job  job=new Job(conf,&quot;myjob&quot;);

        job.setJarByClass(Sort.class);
        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReduce.class);

        // Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;定义Mapper泛型输出类
        job.setMapOutputKeyClass(MyK2.class);
        job.setMapOutputValueClass(LongWritable.class);
        // Reducer&lt;MyK2, LongWritable,LongWritable, LongWritable&gt;定义Reducer泛型输出类
        job.setOutputKeyClass(LongWritable.class);
        job.setOutputValueClass(LongWritable.class);

        FileInputFormat.addInputPath(job,new Path(INPUT_PATH));
        FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH));

        job.waitForCompletion(true);
        System.out.println(&quot;end&quot;);
    }

}
</code></pre>
<h4 id="图案例"><a href="#图案例" class="headerlink" title="图案例"></a>图案例</h4><pre><code>输入：
child    parent 
Tom    Lucy
Tom    Jack
Jone    Lucy
Jone    Jack
Lucy    Mary
Lucy    Ben
Jack     Alice
Jack    Jesse
Terry    Alice
Terry    Jesse
Philip    Terry
Philip    Alma
Mark    Terry
Mark    Alma
需求出输入中的所有的孙子与祖父母


Mapper&lt;LongWritable, Text, Text, Text&gt;
public void map(LongWritable k1, Text v1, Context context)
输入LongWritable k1, Text v1(LongWritable, Text)：序号,行
处理：读取行里的数据split，并以关系形式保存(以Tom    Lucy为例)：
Tom,1,Tom,Lucy
Tom,2,Lucy,Tom
输出Context context(Text, Text)：人名，这个人与其他人的关系(数组)

Reducer&lt;Text, Text, Text, Text&gt;
public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
输入Text key, Iterable&lt;Text&gt; values(Text, Text)：人名，这个人与其他人的关系(数组)
处理：从数组中读出关系并将与该人有关的符合条件的人加入临时数组并输出
输出Context context(Text, Text)：孙子，祖父母

Jone    Alice
Jone    Jesse
Tom    Alice
Tom    Jesse
Jone    Mary
Jone    Ben
Tom    Mary
Tom    Ben
Mark    Alice
Mark    Jesse
Philip    Alice
Philip    Jesse
</code></pre><pre><code class="java">package mr;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem ;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;   

public class MyGL {
    private static class MyGLMapper  extends  Mapper&lt;LongWritable, Text, Text, Text&gt;{  

        public void map(LongWritable k1, Text v1, Context context) 
            throws java.io.IOException, java.lang.InterruptedException{

            //  1   2  file   tab  ,
            String[]  lines = v1.toString().split(&quot;\t&quot;);        

            if(lines.length != 2 || lines[0].trim().equals(&quot;child&quot;))
                return;   //child  parent


            String word1=lines[0].trim();  //  tom
            String word2=lines[1].trim();  //  lucy


            context.write(new Text(word1), new Text(&quot;1&quot;+&quot;,&quot;+word1+&quot;,&quot;+word2));
            context.write(new Text(word2), new Text(&quot;2&quot;+&quot;,&quot;+word1+&quot;,&quot;+word2));
            System.out.println(&quot;map......&quot;+word1+&quot;-&quot;+word2);
        }

    }

    private static class  MyGLReduce extends Reducer&lt;Text, Text, Text, Text&gt;{
        public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
            throws java.io.IOException, java.lang.InterruptedException {
            /*
            * lucy   2+tom+lucy
            * lucy   1+lucy+mary
            * 
            * 2--&gt;split[1]  tom
            * 1--&gt;split[2]  mary
            * 
            * k3=tom  v3=mary
            * */
            List&lt;String&gt;  grandch = new ArrayList();
            List&lt;String&gt;  grandpa = new ArrayList();

            Iterator&lt;Text&gt;  it=values.iterator();
            while(it.hasNext()){
                String  lines= it.next().toString();   //2+tom+lucy
                String[] words=lines.split(&quot;,&quot;);      //[&quot;2&quot;,&quot;tom&quot;,&quot;lucy&quot;]
                if(words[0].equals(&quot;1&quot;)){
                    grandpa.add(words[2]);
                }
                else if(words[0].equals(&quot;2&quot;)){
                    grandch.add(words[1]);

                }
                else
                    return;


            }

            for(String ch:grandch)    
                for(String pa:grandpa){
                    context.write(new Text(ch), new Text(pa)); 
                    System.out.println(&quot;reduce......&quot;+ch+&quot; - &quot;+pa);
                }

            System.out.println(&quot;reduce......&quot;);
        }

       protected void cleanup(Context context) throws java.io.IOException, java.lang.InterruptedException{




        }

    }

    private static String INPUT_PATH=&quot;hdfs://master:9000/input/gl.dat&quot;;
    private static String OUTPUT_PATH=&quot;hdfs://master:9000/output/c/&quot;;

    public static void main(String[] args) throws Exception {    

        Configuration  conf=new Configuration();
        FileSystem  fs=FileSystem.get(new URI(OUTPUT_PATH),conf);

        if(fs.exists(new Path(OUTPUT_PATH)))
                fs.delete(new Path(OUTPUT_PATH));

        Job  job=new Job(conf,&quot;myjob&quot;);

        job.setJarByClass(MyGL.class);
        job.setMapperClass(MyGLMapper.class);
        job.setReducerClass(MyGLReduce.class);


        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);



        FileInputFormat.addInputPath(job,new Path(INPUT_PATH));
        FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH));

        job.waitForCompletion(true);

    }

}
</code></pre>
<!--TODO:http://langyu.iteye.com/blog/992916-->
<!--TODO:http://www.cnblogs.com/zhangchaoyang/articles/2648815.html-->

<h4 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h4><p>矩阵乘法公式：</p>
<img src="矩阵乘法公式.png" />

<pre><code>矩阵A(4*3)(i*n)
1,2,3
4,5,0
7,8,9
10,11,12

矩阵B(3*2)(n*j)
10,15
0,2
11,9

根据矩阵乘法的定义：矩阵A的列数=矩阵B的行数，即矩阵A和矩阵B都有相同的n
矩阵乘法的结果是产生(i*j)的矩阵C

矩阵C(4*2)(i*j)
43,46
40,70
169,202
232,280

1*10+2*0+3*11=43
1*15+2*2+3*9=46

计算每个矩阵C中的元素(i,j)都需要矩阵A的(i,r)与矩阵B的(r,j)相乘再加上下一个r取值[1,n]
接下来看看进行一个矩阵计算需要哪些信息：
因为每次计算r都是从1到n，所以r的值不需要保存进map，
需要：计算结果是在C的哪里即(i,j)，A矩阵对应的值，B矩阵对应的值，这个值来自哪个矩阵(A还是B)

那么如何唯一标识矩阵C的一个元素呢？使用矩阵C的坐标，将C的坐标(i,j)作为key
(哪个矩阵,对应的r,矩阵的值)作为value，这样就可以保存进行矩阵计算的全部信息了

分类讨论：
(i,j为计算C的第(i,j)个元素的值，r取值[1,n])
对于矩阵A的值：
key(i,j) value(a,A的列即r,A[i,r])
对于矩阵B的值：
key(i,j) value(b,B的列即r,B[r,j])

分类讨论的计算过程见下图</code></pre><img src="矩阵乘法.png" />

<pre><code class="java">package demo;

import java.io.IOException;
import java.net.URI;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class MatrixProdect {
    static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {

        private int rowNum = 4;// 矩阵A的行数
        private int colNum = 2;// 矩阵B的列数
        private int rowIndexA = 1; // 矩阵A，当前在第几行
        private int rowIndexB = 1; // 矩阵B，当前在第几行

        public void map(LongWritable key, Text value, Context context)
                throws java.io.IOException, java.lang.InterruptedException {

            FileSplit fs = (FileSplit) context.getInputSplit();
            String fileName = fs.getPath().getName();

            String[] tokens = value.toString().split(&quot;,&quot;); // 读进一行数据
            if (&quot;a&quot;.equals(fileName)) { // 通过文件名判断是矩阵A还是矩阵B
                for (int j = 1; j &lt;= colNum; j++) {
                    Text k = new Text(rowIndexA + &quot;,&quot; + j);
                    for (int r = 0; r &lt; tokens.length; r++) {
                        Text v = new Text(&quot;a,&quot; + (r + 1) + &quot;,&quot; + tokens[r]);
                        System.out.println(&quot;map......&quot; + fileName + &quot;(&quot; + k + &quot;)&quot; + v);
                        context.write(k, v);
                    }
                }
                rowIndexA++;// 每执行一次map方法，扫描矩阵的下一行
            } else if (&quot;b&quot;.equals(fileName)) {
                for (int i = 1; i &lt;= rowNum; i++) {
                    for (int r = 0; r &lt; tokens.length; r++) {
                        Text k = new Text(i + &quot;,&quot; + (r + 1));
                        Text v = new Text(&quot;b,&quot; + rowIndexB + &quot;,&quot; + tokens[r]);
                        System.out.println(&quot;map......&quot; + fileName + &quot;(&quot; + k + &quot;)&quot; + v);
                        context.write(k, v);
                    }
                }
                rowIndexB++;// 每执行一次map方法，扫描矩阵的下一行
            }

        }

    }

    static class MyReduce extends Reducer&lt;Text, Text, Text, IntWritable&gt; {
        public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
                throws java.io.IOException, java.lang.InterruptedException {

            Map&lt;String, String&gt; mapA = new HashMap&lt;String, String&gt;();
            Map&lt;String, String&gt; mapB = new HashMap&lt;String, String&gt;();

            // 根据矩阵来分类
            for (Text value : values) {
                String[] val = value.toString().split(&quot;,&quot;);
                if (&quot;a&quot;.equals(val[0])) {
                    mapA.put(val[1], val[2]);
                } else if (&quot;b&quot;.equals(val[0])) {
                    mapB.put(val[1], val[2]);
                }
            }

            int result = 0;
            Iterator&lt;String&gt; mKeys = mapA.keySet().iterator();
            while (mKeys.hasNext()) { // 取相同的r值的数相乘
                String mkey = mKeys.next();
                if (mapB.get(mkey) == null) {
                    continue;
                }
                result += Integer.parseInt(mapA.get(mkey)) * Integer.parseInt(mapB.get(mkey));
            }
            System.out.println(&quot;reduce......&quot; + &quot;(&quot; + key + &quot;)&quot; + result);
            context.write(key, new IntWritable(result));
        }

    }

    private static String INPUT_PATH_A = &quot;hdfs://master:9000/input/a&quot;;
    private static String INPUT_PATH_B = &quot;hdfs://master:9000/input/b&quot;;
    private static String OUTPUT_PATH = &quot;hdfs://master:9000/output/matrix/&quot;;

    public static void main(String[] args) throws Exception {

        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI(OUTPUT_PATH), conf);

        if (fs.exists(new Path(OUTPUT_PATH)))
            fs.delete(new Path(OUTPUT_PATH));

        Job job = new Job(conf, &quot;myjob&quot;);

        job.setJarByClass(MatrixProdect.class);
        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReduce.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(INPUT_PATH_A));
        FileInputFormat.addInputPath(job, new Path(INPUT_PATH_B));
        FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH));

        job.waitForCompletion(true);
        System.out.println(&quot;end&quot;);
    }

}
</code></pre>
<p>计算方法与上面一样，只是矩阵的存储结构不一样。省略了值为0的元素，对于较大且稀疏的矩阵所占存储空间较小</p>
<pre><code>行,列,值</code></pre><img src="矩阵乘法2.png" />

<!--
```java
// TODO:
```
-->
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2017/08/19/Hadoop/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2017/10/10/Java%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Java复习笔记
          
        </div>
      </a>
    
    
      <a href="/2017/08/17/Hive/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Hive</div>
      </a>
    
  </nav>

   
 
   
<div class="gitalk" id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script>
<script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: 'db290dc011893521d885',
    clientSecret: '4cbc64d2114f4963d8090a55b508568b2f26075b',
    repo: 'blog-comments',
    owner: 'Sicmatr1x',
    admin: ['Sicmatr1x'],
    // id: location.pathname,      // Ensure uniqueness and length less than 50
    id: md5(location.pathname),
    distractionFreeMode: false,  // Facebook-like distraction free mode
    pagerDirection: 'last'
  })

  gitalk.render('gitalk-container')
</script>
  
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2020
        <i class="ri-heart-fill heart_icon"></i> Sicmatr1x
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Sicmatr1x"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<!-- Tocbot -->

<script src="/js/tocbot.min.js"></script>
<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>
<script src="/dist/main.js"></script>
<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->

<script src="/js/busuanzi-2.3.pure.min.js"></script>

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->

<link rel="stylesheet" href="/css/clipboard.css">
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>
