<!DOCTYPE html><html lang="en" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Hadoop | Sicmatr1x's Blog</title><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - 共 $1 行","copy":"复制","copyFinish":"复制成功","expand":"展开"}}</script><script src="/js/gitalk.js"></script><script src="//unpkg.com/mermaid@9.2.2/dist/mermaid.min.js"></script><link rel="stylesheet" href="/css/arknights.css"><script>if (window.localStorage.getItem('theme-mode') === 'light') document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark') document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Sicmatr1x's Blog" type="application/atom+xml">
</head><body><div class="loading" style="opacity: 0"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><nav><div class="navBtn hide"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a class="navBlock" href="/toolkit/"><span class="navItemTitle">Toolkit</span></a></li><li class="navItem"><a class="navBlock" href="/about/"><span class="navItemTitle">About</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>Hadoop</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2022-08-19T09:38:04.000Z" id="date"> 2022-08-19</time></div></span><br><span>最后更新时间: <div class="control"><time datetime="2023-03-26T08:08:46.332Z" id="updated"> 2023-03-26</time></div></span></div></div><hr><div id="post-content"><h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><h3 id="Hadoop："><a href="#Hadoop：" class="headerlink" title="Hadoop："></a>Hadoop：</h3><p>Hadoop是一个框架，它是由Java语言来实现的。Hadoop是处理大数据技术.Hadoop可以处理云计算产生大数据。</p>
<p>CDH商业版：</p>
<p>Cloudera CDH是Hadoop的一个版本，比Apache Hadoop的优点如下：</p>
<ol>
<li>CDH基于稳定版Apache Hadoop，并应用了最新Bug修复或者Feature的Patch。Cloudera常年坚持季度发行Update版本，年度发行Release版本，更新速度比Apache官方快，而且在实际使用过程中CDH表现无比稳定，并没有引入新的问题。</li>
<li>Cloudera官方网站上安装、升级文档详细，省去Google时间。</li>
<li>CDH支持Yum&#x2F;Apt包，Tar包，RPM包，Cloudera Manager四种方式安装，总有一款适合您。官方网站推荐Yum&#x2F;Apt方式安装，其好处如下：</li>
</ol>
<ul>
<li>联网安装、升级，非常方便。当然你也可以下载rpm包到本地，使用Local Yum方式安装。</li>
<li>自动下载依赖软件包，比如要安装Hive，则会级联下载、安装Hadoop。</li>
<li>Hadoop生态系统包自动匹配，不需要你寻找与当前Hadoop匹配的Hbase，Flume，Hive等软件，Yum&#x2F;Apt会根据当前安装Hadoop版本自动寻找匹配版本的软件包，并保证兼容性。</li>
<li>自动创建相关目录并软链到合适的地方（如conf和logs等目录）；自动创建hdfs, mapred用户，hdfs用户是HDFS的最高权限用户，mapred用户则负责mapreduce执行过程中相关目录的权限。</li>
</ul>
<h3 id="大数据的4个V"><a href="#大数据的4个V" class="headerlink" title="大数据的4个V:"></a>大数据的4个V:</h3><ol>
<li>Velocity：实现快速的数据流传</li>
<li>Variety： 具有多样的数据类型</li>
<li>Volume： 存有海量的数据规模（TB，PB，EB级别）</li>
<li>Value：存在着巨大的价值</li>
</ol>
<hr>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><img src="Hadoop项目结构图.jpg" />

<p>Hadoop实际上就是谷歌三宝的开源实现，</p>
<p>Hadoop MapReduce对应Google MapReduce，</p>
<p>HBase对应BigTable，</p>
<p>HDFS对应GFS。HDFS（或GFS）为上层提供高效的非结构化存储服务，</p>
<p>HBase（或BigTable）是提供结构化数据服务的分布式数据库，Hadoop MapReduce（或Google MapReduce）是一种并行计算的编程模型，用于作业调度。</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。</p>
<h3 id="HDFS-Hadoop-Distributed-File-System-："><a href="#HDFS-Hadoop-Distributed-File-System-：" class="headerlink" title="HDFS(Hadoop Distributed File System)："></a>HDFS(Hadoop Distributed File System)：</h3><ul>
<li>默认的最基本的存储单位是64M的数据块。</li>
<li>和普通文件系统相同的是，HDFS中的文件是被分成64M一块的数据块存储的。</li>
<li>不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。</li>
</ul>
<h3 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h3><p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。</p>
<h3 id="联机事务处理OLTP-On-line-Transaction-Processing-、联机分析处理OLAP-On-Line-Analytical-Processing"><a href="#联机事务处理OLTP-On-line-Transaction-Processing-、联机分析处理OLAP-On-Line-Analytical-Processing" class="headerlink" title="联机事务处理OLTP(On-line Transaction Processing)、联机分析处理OLAP(On-Line Analytical Processing)"></a>联机事务处理OLTP(On-line Transaction Processing)、联机分析处理OLAP(On-Line Analytical Processing)</h3><p>OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 </p>
<img src="OLTP与OLAP之间的比较.jpg" />

<p>分析型数据不允许update、delete操作</p>
<h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><p>Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
<h3 id="ZooKepper"><a href="#ZooKepper" class="headerlink" title="ZooKepper"></a>ZooKepper</h3><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p>
<p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。<br>ZooKeeper包含一个简单的原语集，提供Java和C的接口。</p>
<h3 id="Mahout"><a href="#Mahout" class="headerlink" title="Mahout"></a>Mahout</h3><img src="Mahout.PNG" />

<p>Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。</p>
<h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><h3 id="支持平台"><a href="#支持平台" class="headerlink" title="支持平台"></a>支持平台</h3><ul>
<li>GNU&#x2F;Linux是产品开发和运行的平台。Hadoop已在有2000个节点的GNU&#x2F;Linux主机组成的集群系统上得到验证。</li>
<li>Win32平台是作为<code>开发平台</code>支持的。由于分布式操作尚未在Win32平台上充分测试，所以还不作为一个<code>生产平台</code>被支持。</li>
</ul>
<h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><h4 id="安装-VMware"><a href="#安装-VMware" class="headerlink" title="安装 VMware"></a>安装 VMware</h4><h4 id="安装-Ubuntu"><a href="#安装-Ubuntu" class="headerlink" title="安装 Ubuntu"></a>安装 Ubuntu</h4><h4 id="安装-jdk"><a href="#安装-jdk" class="headerlink" title="安装 jdk"></a>安装 jdk</h4><p>解压<code>tar -vzfx jdk-1.7.0.tar.gz</code></p>
<p>配环境变量<code>sudo vim /etc/profile</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> JAVA_HOME=/home/master0/Desktop/jkd1.7.0_80<br><span class="hljs-built_in">export</span> CLASSPATH=.:<span class="hljs-variable">$JAVA_HOME</span>/lib/dt.jar:<span class="hljs-variable">$JAVA_HOME</span>/lib/tools.jar<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$JAVA_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure>

<p>使配置文件生效：<code>source /etc/profile</code></p>
<h4 id="安装Hadoop-1"><a href="#安装Hadoop-1" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><p>配环境变量<code>sudo vim /etc/profile</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> JAVA_HOME=/home/master0/Desktop/jdk1.7.0_80<br><span class="hljs-built_in">export</span> HADOOP_HOME=/home/master0/Desktop/hadoop-2.6.0<br><span class="hljs-built_in">export</span> CLASSPATH=.:<span class="hljs-variable">$JAVA_HOME</span>/lib/dt.jar:<span class="hljs-variable">$JAVA_HOME</span>/lib/tools.jar:<span class="hljs-variable">$HADOOP_HOME</span>/share/hadoop/common/hadoop-common-2.6.0.jar:<span class="hljs-variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:<span class="hljs-variable">$HADOOP_HOME</span>/share/hadoop/common/lib/commons-1.2.jar:<span class="hljs-variable">$CLASSPATH</span><br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$JAVA_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$HADOOP_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure>

<p>配置<code>~/hadoop-2.6.0/etc/hadoop/hadoop-env.sh</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># The java implementation to use.</span><br><span class="hljs-built_in">export</span> JAVA_HOME=/home/master0/Desktop/jdk1.7.0_80<br></code></pre></td></tr></table></figure>

<h4 id="测试hadoop"><a href="#测试hadoop" class="headerlink" title="测试hadoop"></a>测试hadoop</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop version<br></code></pre></td></tr></table></figure>

<hr>
<h4 id="使用hadoop的本地单独模式"><a href="#使用hadoop的本地单独模式" class="headerlink" title="使用hadoop的本地单独模式"></a>使用hadoop的本地单独模式</h4><p>对某目录下的文档进行单词数的统计</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">cd</span>  /home/hadoop/	<br>$ <span class="hljs-built_in">mkdir</span>  input<br>$ <span class="hljs-built_in">cp</span>   <span class="hljs-variable">$HADOOP_HOME</span>/etc/hadoop/*.xml   input/<br>$ hadoop  jar <span class="hljs-variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output <span class="hljs-string">&#x27;dfs[a-z.]+&#x27;</span><br>$ <span class="hljs-built_in">cat</span> output/*<br></code></pre></td></tr></table></figure>

<h4 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h4><p>修改主机名</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo gedit /etc/hostname<br></code></pre></td></tr></table></figure>

<hr>
<h4 id="配置静态IP"><a href="#配置静态IP" class="headerlink" title="配置静态IP"></a>配置静态IP</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo gedit /etc/network/interfaces<br></code></pre></td></tr></table></figure>

<p>编辑-&gt;虚拟网络编辑器-&gt;查看NAT模式的子网地址</p>
<p>例如为：231</p>
<p>master</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">auto eth0<br>iface eth0 inet static<br>address 192.168.231.129<br>netmask 255.255.255.0<br>network 192.168.231.0<br>boardcast 192.168.231.255<br>gateway 192.168.231.2<br>dns-nameservers 192.168.1.1 8.8.8.8 8.8.8.4<br><br>ping 192.168.231.130<br></code></pre></td></tr></table></figure>

<p>serve1</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">auto eth0<br>iface eth0 inet static<br>address 192.168.231.130<br>netmask 255.255.255.0<br>network 192.168.231.0<br>boardcast 192.168.231.255<br>gateway 192.168.231.2<br>dns-nameservers 192.168.1.1 8.8.8.8 8.8.8.4<br><br>ping 192.168.231.129<br></code></pre></td></tr></table></figure>

<p>若访问不了网页的话可以将物理机的dns填写在dns-nameservers第一个</p>
<p>若拖文件拖不进虚拟机需检查：</p>
<p>虚拟机ping与其对应的模式的虚拟网卡可不可以ping通</p>
<p>主机ping与虚拟机可不可以ping通</p>
<p>VMware Network Adapter VMnet1:桥接模式虚拟网卡</p>
<p>VMware Network Adapter VMnet8:NAT模式虚拟网卡</p>
<h4 id="修改hosts文件"><a href="#修改hosts文件" class="headerlink" title="修改hosts文件"></a>修改hosts文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo gedit /etc/hosts<br><br>192.168.231.129  master<br>192.168.231.130  serve1<br>192.168.231.131  serve2<br></code></pre></td></tr></table></figure>

<h4 id="安装ssh"><a href="#安装ssh" class="headerlink" title="安装ssh"></a>安装ssh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get install ssh<br></code></pre></td></tr></table></figure>

<p>安装完毕就会出现<code>/home/master/.ssh</code>文件夹</p>
<p>然后需要生成了一个公钥</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa -P <span class="hljs-string">&#x27;&#x27;</span><br><br>会生成<br>id_rsa  id_rsa.pub  known_hosts<br></code></pre></td></tr></table></figure>

<p>id_rsa为私钥</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">-----BEGIN RSA PRIVATE KEY-----<br>MIIEpAIBAAKCAQEAp6JLYxM9lm/ciNG5SuAd/0WBBY0VN98w1KLad5GrkZhM5iZ1<br>mKnl1JHhT14//QSqtJ/tAAo8P1EZspvziS1q77DVBF4L/kInl0KEZOiFWMUOKqDj<br>y+TWLSZmBK9uP5J2cb2wnIMZ4HeWw0y8hnaCpfg4FNVm8WL/EQh++EHx4VBQv0bt<br>4s3qZ9LgYM0MGDrizYKCZ92vRE2CVgLlpAzXvD2uFfxlFwJl02l35fjaIW2ed6PV<br>HrnT8D6BrpUIdKWzWsevj3W6IfO0upBtqOygJw0RxYSx646nDDdFXIk7bzdVFXdr<br>sOjOblsPGqTJs+aApEQB3avOUZI0EixCr2h7/wIDAQABAoIBAQCY1je1lQ1J46NG<br>ezBdPAkdfNktnnwB/NQginp1GbM7g4hZLid5kS2iqX6rRltA7MhW9pi2uJ5FfEPZ<br>vKZGI8qjzq3o1XZJ0zcVief7uKQbU06fPyFx/KnpcGEDVI9IFtk2yqQDjuRA68fh<br>OE2KqvJjL/Sxyf+ZhZDYjs50ums16PHxXlhAaP8EI78Dcff5sx+ZoKTVGum4Jrdl<br>h0cXeDBcxJZg7wEtHPEUrduaiwEv88fD7aw2QwsYdCuPECncltR57iPi95hr7uaW<br>XdtRZ+mAey5sBxJmZKrlPE6kK3yAvSs1tP0yz4R4azAYQqTpLmxcfMrqWRwb3IMA<br>9Rl98FIBAoGBANpNaYJgaTvDT2Nski6sTu1oPefow4tosvPE1jZ/gWXExJ9m1OiI<br>GcZGG0nM+UCx85+//B6gyLdvmUGgxN9vzmY3myuhQ9iesep7W+DiqDnz2J/VRM98<br>eEso6P1jevlC90WJh1wNrVIuzxuN/5A5LghjNNuHCnZzJTRuSKjISjRhAoGBAMSU<br>9hdNDliOXDIIRs/vjwiRuLvbECMFqETSyFdnc91dAi2cYfwlfKFlWGSPFO/LuTvL<br>9PfWaKgfuAzMiZ5JoMPlo5iX8atX1V4Naz7e3OBR9rhyD0oO4aNyKtvDv7tIWTxm<br>eWw/4hlmPp/wGYgfxlPOrbVfJcESYk9FmRxxeoxfAoGAP87ozCcKG2HXTqRphiLv<br>Xw1dKvAqWBFeXUpnor5aQDjnkAAqs100y3OqfkPfhz18jHE9bGZqxNNl5HztjrHL<br>jq0qOfKFNkgMkRFFpdIagfX4l59q4YrsTmvCzm3JgBpG1JiCbDHDO4ZbGx7CWJGe<br>Fu2IgbJTKJQ3h7/ElTEWH4ECgYEAoxOr/vJ2hzI5+2twSwlBT+uLI5P8FAGacNWn<br>SxLQRH/m0a2cf48dj8pCBNHJnZAUby2oX30nvujpRvza4UvVKQ20pF7QJcMshuR8<br>5l/9Pb3g/WvpkRc9SdjpAvylbpj7JicgbZOlXkq6gvWsSIeLgHTBF+gBquQ0V+y1<br>sqnU7uMCgYBMSR2QDG5TuSp7pNOFBFuqhOCrUHZmKqoHCZ7rSh3etxc8D5tLXciE<br>APNWfGqSE2aT/PJgqNoxl5p42bnZrv3cXJuiD9Wid6yFzDH0oUa9K66vy1SWV+B8<br>3rHha5wLzizgNUQZjh1XSndp1WekYCLjV+Bn8b/odBClcHKX7M/BOg==<br>-----END RSA PRIVATE KEY-----<br></code></pre></td></tr></table></figure>

<p>id_rsa.pub为公钥</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnoktjEz2Wb9yI0blK4B3/RYEFjRU33zDUotp3kauRmEzmJnWYqeXUkeFPXj/9BKq0n+0ACjw/URmym/OJLWrvsNUEXgv+QieXQoRk6IVYxQ4qoOPL5NYtJmYEr24/knZxvbCcgxngd5bDTLyGdoKl+DgU1WbxYv8RCH74QfHhUFC/Ru3izepn0uBgzQwYOuLNgoJn3a9ETYJWAuWkDNe8Pa4V/GUXAmXTaXfl+NohbZ53o9UeudPwPoGulQh0pbNax6+Pdboh87S6kG2o7KAnDRHFhLHrjqcMN0VciTtvN1UVd2uw6M5uWw8apMmz5oCkRAHdq85RkjQSLEKvaHv/ master@ubuntu<br></code></pre></td></tr></table></figure>

<p>要想免密登录则需要被免密登录方的公钥：这里可以先将各台分机的公钥发送给主机master，然后再由master合成一个文件再发送给分机。这样每台机器都会有其它所有机器的公钥</p>
<p>生成公钥文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys<br></code></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">authorized_keys如下，其实和公钥相同：<br>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnoktjEz2Wb9yI0blK4B3/RYEFjRU33zDUotp3kauRmEzmJnWYqeXUkeFPXj/9BKq0n+0ACjw/URmym/OJLWrvsNUEXgv+QieXQoRk6IVYxQ4qoOPL5NYtJmYEr24/knZxvbCcgxngd5bDTLyGdoKl+DgU1WbxYv8RCH74QfHhUFC/Ru3izepn0uBgzQwYOuLNgoJn3a9ETYJWAuWkDNe8Pa4V/GUXAmXTaXfl+NohbZ53o9UeudPwPoGulQh0pbNax6+Pdboh87S6kG2o7KAnDRHFhLHrjqcMN0VciTtvN1UVd2uw6M5uWw8apMmz5oCkRAHdq85RkjQSLEKvaHv/ master@ubuntu<br></code></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">分机serve1复制公钥到master主机上：<br>scp .ssh/id_rsa.pub master@master:/home/master/id_rsa_1.pub<br>将分机serve1的公钥追加到主机的authorized_keys上<br><span class="hljs-built_in">cat</span> id_rsa_1.pub &gt;&gt; .ssh/authorized_keys<br></code></pre></td></tr></table></figure>

<p>重复以上两步直到主机master的authorized_keys有所有分机的公钥，再进行分发操作</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">scp .ssh/authorized_keys master@serve1:/home/master/.ssh/authorized_keys<br>scp .ssh/authorized_keys master@serve2:/home/master/.ssh/authorized_keys<br></code></pre></td></tr></table></figure>

<p>分发完毕后即可进行测试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh master<br>ssh serve1<br>能连接成功即可<br></code></pre></td></tr></table></figure>

<p>SSH免密码设置失败解决</p>
<ol>
<li>权限问题</li>
</ol>
<p>.ssh目录，以及&#x2F;home&#x2F;当前用户 需要700权限，参考以下操作调整</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$sudo</span>   <span class="hljs-built_in">chmod</span>   777   ~/.ssh<br><br><span class="hljs-variable">$sudo</span>  <span class="hljs-built_in">chmod</span> 700  /home/当前用户<br></code></pre></td></tr></table></figure>

<p>.ssh目录下的authorized_keys文件需要600或644权限，参考以下操作调整</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$sudo</span> <span class="hljs-built_in">chmod</span>   644   ~/.ssh/authorized_keys<br></code></pre></td></tr></table></figure>
<ol start="2">
<li>StrictModes问题</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$sudo</span> gedit /etc/ssh/sshd_config<br></code></pre></td></tr></table></figure>

<p>找到</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">\<span class="hljs-comment">#StrictModes yes</span><br></code></pre></td></tr></table></figure>

<p>改成</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">StrictModes no<br></code></pre></td></tr></table></figure>
<p>如果还不行，可以用<code>ssh -vvv 目标机器ip</code> 查看详情</p>
<h4 id="配置Hadoop集群"><a href="#配置Hadoop集群" class="headerlink" title="配置Hadoop集群"></a>配置Hadoop集群</h4><p>以下将会修改多个Hadoop配置文件均位于<code>hadoop-2.6.0/etc</code>目录下</p>
<p>修改：<code>hadoop-env.sh</code> 、<code>yarn-env.sh</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">gedit etc/hadoop/hadoop-env.sh<br><br><span class="hljs-comment"># The java implementation to use.</span><br><span class="hljs-built_in">export</span> JAVA_HOME=/home/master/jdk1.7.0_80<br></code></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">gedit etc/hadoop/yarn-env.sh<br></code></pre></td></tr></table></figure>

<p>core-site.xml</p>
<p>core-site.xml的完整参数请参考: <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml">http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">gedit etc/hadoop/core-site.xml<br></code></pre></td></tr></table></figure>

<p><code>/home/hadoop/tmp</code> 目录如不存在，则先mkdir手动创建</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://master:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-comment">&lt;!--主机名:端口号--&gt;</span>     <br> <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>     <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>     <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/master/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-comment">&lt;!--/tmp/hadoop-$&#123;user.name&#125;--&gt;</span>   <br> <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> <br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>hdfs-site.xml</p>
<p>hdfs-site.xml的完整参数请参考: <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">gedit etc/hadoop/hdfs-site.xml<br></code></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.ipc.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>0.0.0.0:50020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.http.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>0.0.0.0:50075<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/home/master/data/namenode<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-comment">&lt;!--元数据--&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/home/master/data/datanode<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-comment">&lt;!--数据块--&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>slave1:9001<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-comment">&lt;!--备份数量--&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-comment">&lt;!--权限验证--&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>配置slaves分机列表</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">gedit etc/hadoop/slaves<br></code></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">master<br>serve1<br></code></pre></td></tr></table></figure>

<p>分发配置文件到集群的其它机器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scp -r hadoop-2.6.0/etc/hadoop/ master@serve1:/home/master/hadoop-2.6.0/etc/<br></code></pre></td></tr></table></figure>

<p>格式化hdfs</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hdfs namenode -format<br></code></pre></td></tr></table></figure>

<p>等看到执行信息有has been successfully formatted表示格式化ok</p>
<p>启动 dfs</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop-2.6.0/sbin/start-dfs.sh<br></code></pre></td></tr></table></figure>

<p>验证hadoop是否启动成功：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$jps</span><br>显示有：<br>4895 DataNode<br>4775 NameNode<br></code></pre></td></tr></table></figure>

<h4 id="安装-MapReduce"><a href="#安装-MapReduce" class="headerlink" title="安装 MapReduce"></a>安装 MapReduce</h4><p>mapred-site.xml的完整参数请参考<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></p>
<p>将mapred-site.xml.template改名成mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:10020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:19888<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>yarn-site.xml</p>
<p>yarn-site.xml的完整参数请参考: <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">http://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:8030<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:8025<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:8040<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<h4 id="启动yarn"><a href="#启动yarn" class="headerlink" title="启动yarn"></a>启动yarn</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop-2.6.0/sbin/start-yarn.sh<br></code></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$jps</span><br>多了ResourceManager和NodeManager表示启动yarn成功<br>SecondaryNameNode<br>ResourceManager<br>NameNode<br></code></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop-2.6.0/sbin/start-dfs.sh<br>hadoop-2.6.0/sbin/start-yarn.sh<br><br>jps<br>master节点上有几下3个进程：<br>7482 ResourceManager<br>7335 SecondaryNameNode<br>7159 NameNode<br>slave1、slave2上有几下2个进程：<br>2296 DataNode<br>2398 NodeManager<br><br>hadoop-2.6.0/sbin/stop-dfs.sh<br>hadoop-2.6.0/sbin/stop-yarn.sh<br></code></pre></td></tr></table></figure>

<p>或打开浏览器访问：hdfs管理界面: <a target="_blank" rel="noopener" href="http://master:50070/">http://master:50070</a></p>
<p>yarn的管理界面: <a target="_blank" rel="noopener" href="http://master:8088/">http://master:8088/</a></p>
<p>查看hadoop状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">hdfs dfsadmin -report //查看hdfs的状态报告<br><br>yarn  node -list   //查看yarn的基本信息<br><br>Secondary// TODO<br>NameNode 元数据<br>DataNode 数据块<br></code></pre></td></tr></table></figure>

<hr>
<h2 id="HDFS文件系统"><a href="#HDFS文件系统" class="headerlink" title="HDFS文件系统"></a>HDFS文件系统</h2><p>hadoop实现了一个分布式文件系统HDFS(Hadoop Distributed File System)</p>
<img src="HDFS架构.png" />

<p>元数据：用于描述数据的数据。</p>
<p>NameNode 主服务器，用来管理整个文件系统的命名空间和元数据，以及处理来自外界的文件访问请求。整个集群中只有一个。含有：</p>
<ol>
<li>命名空间：整个分布式文件系统的目录结构</li>
<li>数据块与文件名的映射表</li>
<li>每个数据块副本的位置信息(每个数据块默认3个副本)</li>
</ol>
<p>元数据保存在NameNode的内存当中(1G内存可存放1000000个块对应的元数据信息，缺省每块64M计算可对应64T实际数据)</p>
<p>DataNode通过心跳包(Heartbeats)与NameNode通讯</p>
<p>HA(High Available)高可用</p>
<p>DataNode 用来实际存储和管理文件的数据块</p>
<p>数据块-64M(128M)数据块+备份公用一个ID</p>
<p>主从架构：1个NameNode对应n个DataNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>client-java app -&gt; data NameNode(客户端向NameNode发起请求)<br>client-sid datanode-&gt; datanode -&gt; r/w -&gt; dfs file(NameNode返回对应的DataNode给客户端让客户端来通过DataNode进行访问)<br>                   -&gt; namenode(向NameNode汇报情况)<br></code></pre></td></tr></table></figure>

<h3 id="JVM从HDFS读取文件流程"><a href="#JVM从HDFS读取文件流程" class="headerlink" title="JVM从HDFS读取文件流程"></a>JVM从HDFS读取文件流程</h3><img src="HDFS数据的读取过程.png" />

<p>client会从距离最近的机子上读取</p>
<h4 id="HDFS文件存储的组织与读写："><a href="#HDFS文件存储的组织与读写：" class="headerlink" title="HDFS文件存储的组织与读写："></a>HDFS文件存储的组织与读写：</h4><p>数据写入</p>
<ol>
<li>客户端调用FileSystem 实例的create 方法，创建文件。NameNode 通过一些检查，比如文件是否存在，客户端是否拥有创建权限等;通过检查之后，在NameNode 添加文件信息。注意，因为此时文件没有数据，所以NameNode 上也没有文件数据块的信息。</li>
<li>创建结束之后， HDFS 会返回一个输出流DFSDataOutputStream 给客户端。</li>
<li>客户端调用输出流DFSDataOutputStream 的write 方法向HDFS 中对应的文件写入数据。</li>
<li>数据首先会被分包，这些分包会写人一个输出流的内部队列Data 队列中，接收完数据分包，输出流DFSDataOutputStream 会向NameNode 申请保存文件和副本数据块的若干个DataNode ， 这若干个DataNode 会形成一个数据传输管道。DFSDataOutputStream 将数据传输给距离上最短的DataNode ，这个DataNode 接收到数据包之后会传给下一个DataNode 。数据在各DataNode之间通过管道流动，而不是全部由输出流分发，以减少传输开销。</li>
<li>因为各DataNode 位于不同机器上，数据需要通过网络发送，所以，为了保证所有DataNode 的数据都是准确的，接收到数据的DataNode 要向发送者发送确认包(ACK Packet ) 。对于某个数据块，只有当DFSDataOutputStream 收到了所有DataNode 的正确ACK. 才能确认传输结束。DFSDataOutputStream 内部专门维护了一个等待ACK 队列，这一队列保存已经进入管道传输数据、但是并未被完全确认的数据包。</li>
<li>不断执行第3 - 5 步直到数据全部写完，客户端调用close 关闭文件。</li>
<li>DFSDataInputStream 继续等待直到所有数据写人完毕并被确认，调用complete 方法通知NameNode 文件写入完成。NameNode 接收到complete 消息之后，等待相应数量的副本写入完毕后，告知客户端</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">查看文件<br>hadoop fs -<span class="hljs-built_in">cat</span> /output/part-00000<br>查看hadoop文件系统<br>hadoop fs -<span class="hljs-built_in">ls</span> /<br>hadoop fs -<span class="hljs-built_in">ls</span> -R /output<br>hadoop fs -<span class="hljs-built_in">ls</span> /output<br>创建文件夹<br>hadoop fs -<span class="hljs-built_in">mkdir</span> /tmp<br>hadoop fs -<span class="hljs-built_in">mkdir</span> /input<br>hadoop fs -<span class="hljs-built_in">mkdir</span> /output<br>将文件放到hadoop文件系统-put 当前路径 /home/master/input 放到的路径<br>hadoop fs -put /home/master/input/* /input<br>hadoop fs -get /output output<br><br>hadoop fs -<span class="hljs-built_in">rm</span> -R /input<br>hadoop fs -<span class="hljs-built_in">rm</span> -r /output/output<br>hadoop fs -<span class="hljs-built_in">mv</span> /output/output/part-r-00000 /output/part-r-00000<br></code></pre></td></tr></table></figure>

<p>运行例子</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop jar hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep /input /output <span class="hljs-string">&#x27;dfs[a-z.]+&#x27;</span><br></code></pre></td></tr></table></figure>

<h3 id="Hadoop-IO"><a href="#Hadoop-IO" class="headerlink" title="Hadoop IO"></a>Hadoop IO</h3><p>HDFS数据完整性</p>
<p>校验和+后台进程</p>
<p>文件数据结构-解决大量小文件</p>
<p>SequenceFile：用流来读写</p>
<p>MapFile</p>
<h2 id="MapReduce-1"><a href="#MapReduce-1" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>Map&#x2F;Reduce是一个用于大规模数据处理的分布式计算模型，它最初是由Google工程师设计并实现的，Google已经将它完整的MapReduce论文公开发布了。其中对它的定义是，Map&#x2F;Reduce是一个编程模型（programming model），是一个用于处理和生成大规模数据集（processing and generating large data sets）的相关的实现。用户定义一个map函数来处理一个key&#x2F;value对以生成一批中间的key&#x2F;value对，再定义一个reduce函数将所有这些中间的有着相同key的values合并起来。很多现实世界中的任务都可用这个模型来表达。</p>
<img src="how-hadoop-runs-a-mapreduce-kob.jpg" />

<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><img src="hadoop-mapreduce-framework-architecture.jpg" />

<p>Mapper、Reduce</p>
<blockquote>
<p>运行于Hadoop的MapReduce应用程序最基本的组成部分包括一个Mapper和一个Reducer类，以及一个创建JobConf的执行程序，在一些应用中还可以包括一个Combiner类，它实际也是Reducer的实现。</p>
</blockquote>
<p>JobTracker、TaskTracker</p>
<blockquote>
<p>它们都是由一个master服务JobTracker和多个运行于多个节点的slaver服务TaskTracker两个类提供的服务调度的。master负责调度job的每一个子任务task运行于slave上，并监控它们，如果发现有失败的task就重新运行它，slave则负责直接执行每一个task。TaskTracker都需要运行在HDFS的DataNode上，而JobTracker则不需要，一般情况应该把JobTracker部署在单独的机器上。</p>
</blockquote>
<p>JobClient</p>
<blockquote>
<p>每一个job都会在用户端通过JobClient类将应用程序以及配置参数Configuration打包成jar文件存储在HDFS，并把路径提交到JobTracker的master服务，然后由master创建每一个Task（即MapTask和ReduceTask）将它们分发到各个TaskTracker服务中去执行。</p>
</blockquote>
<p>JobInProgress</p>
<blockquote>
<p>JobClient提交job后，JobTracker会创建一个JobInProgress来跟踪和调度这个job，并把它添加到job队列里。JobInProgress会根据提交的job jar中定义的输入数据集（已分解成FileSplit）创建对应的一批TaskInProgress用于监控和调度MapTask，同时在创建指定数目的TaskInProgress用于监控和调度ReduceTask，缺省为1个ReduceTask。</p>
</blockquote>
<p>TaskInProgress</p>
<blockquote>
<p>JobTracker启动任务时通过每一个TaskInProgress来launchTask，这时会把Task对象（即MapTask和ReduceTask）序列化写入相应的TaskTracker服务中，TaskTracker收到后会创建对应的TaskInProgress（此TaskInProgress实现非JobTracker中使用的TaskInProgress，作用类似）用于监控和调度该Task。启动具体的Task进程是通过TaskInProgress管理的TaskRunner对象来运行的。TaskRunner会自动装载job jar，并设置好环境变量后启动一个独立的java child进程来执行Task，即MapTask或者ReduceTask，但它们不一定运行在同一个TaskTracker中。</p>
</blockquote>
<p>MapTask、ReduceTask</p>
<blockquote>
<p>一个完整的job会自动依次执行Mapper、Combiner（在JobConf指定了Combiner时执行）和Reducer，其中Mapper和Combiner是由MapTask调用执行，Reducer则由ReduceTask调用，Combiner实际也是Reducer接口类的实现。Mapper会根据job jar中定义的输入数据集按&lt;key1,value1&gt;对读入，处理完成生成临时的&lt;key2,value2&gt;对，如果定义了Combiner，MapTask会在Mapper完成调用该Combiner将相同key的值做合并处理，以减少输出结果集。MapTask的任务全完成即交给ReduceTask进程调用Reducer处理，生成最终结果&lt;key3,value3&gt;对。这个过程在下一部分再详细介绍。</p>
</blockquote>
<img src="mapreduce运行机制.jpg" />

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><h4 id="单词统计案例"><a href="#单词统计案例" class="headerlink" title="单词统计案例"></a>单词统计案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Mapper&lt;LongWritable, Text, Text, IntWritable&gt;<br>public void map(LongWritable k1, Text v1, Context context)<br>输入LongWritable k1, Text v1(LongWritable, Text)：序号,行<br>处理：从行中split出每个单词，并将每个单词的值设为1<br>输出Context context(Text, IntWritable)：单词,所有该单词的值的集合(数组)<br><br>Reducer&lt;Text, IntWritable, Text, IntWritable&gt;<br>public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)<br>输入Text key, Iterable&lt;IntWritable&gt; values(Text, IntWritable)：单词,所有该单词的值的集合(数组)<br>处理：使用迭代器Iterator来迭代每个单词的值的数组并将数组中的每个元素相加，和作为该单词新的值<br>输出Context context(Text, IntWritable)：单词,单词出现次数<br></code></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> mypro1;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.net.URI;<br><span class="hljs-keyword">import</span> java.util.Iterator;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;   <br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyWordCount</span> &#123;<br><br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyMapper</span>  <span class="hljs-keyword">extends</span>  <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt;&#123;  <br>		<span class="hljs-comment">// 输入LongWritable k1, Text v1(LongWritable, Text)：序号,行</span><br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable k1, Text v1, Context context)</span> <br>			<span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException&#123;<br>			<span class="hljs-comment">// 处理：从行中split出每个单词，并将每个单词的值设为1</span><br>			String[]  lines= v1.toString().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>			<span class="hljs-keyword">for</span>(String word: lines)&#123;<br>				<span class="hljs-comment">// 输出Context context(Text, IntWritable)：单词,所有该单词的值的集合(数组)</span><br>				context.write(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(word), <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(<span class="hljs-number">1</span>));<br>			&#125;<br>			<br>			System.out.println(<span class="hljs-string">&quot;map......&quot;</span>);<br>		&#125;<br>		<br>	&#125;<br>	<br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span>  <span class="hljs-title class_">MyReduce</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt;&#123;<br>		<span class="hljs-comment">// 输入Text key, Iterable&lt;IntWritable&gt; values(Text, IntWritable)：单词,所有该单词的值的集合(数组)</span><br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span><br>		 	<span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException&#123;<br>			<span class="hljs-comment">// 处理：使用迭代器Iterator来迭代每个单词的值的数组并将数组中的每个元素相加，和作为该单词新的值</span><br>			<span class="hljs-type">int</span> sum=<span class="hljs-number">0</span>;<br>			Iterator&lt;IntWritable&gt;  it = values.iterator();<br>			<span class="hljs-keyword">while</span>(it.hasNext())&#123;<br>				sum+= it.next().get();<br>			&#125;<br>			<span class="hljs-comment">// 输出Context context(Text, IntWritable)：单词,单词出现次数</span><br>			context.write(key, <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(sum));    <br>			 <br>			System.out.println(<span class="hljs-string">&quot;reduce......&quot;</span>);<br>		&#125;<br>		    <br>	&#125;<br><br>	<span class="hljs-comment">// 定义输入文件</span><br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String INPUT_PATH=<span class="hljs-string">&quot;hdfs://master:9000/input/hdfs-site.xml&quot;</span>;<br>	<span class="hljs-comment">// 定义输出结果到目录</span><br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String OUTPUT_PATH=<span class="hljs-string">&quot;hdfs://master:9000/output/c/&quot;</span>;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;	<br>		<span class="hljs-comment">// 加载配置文件</span><br>		Configuration  conf=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>		FileSystem  fs=FileSystem.get(<span class="hljs-keyword">new</span> <span class="hljs-title class_">URI</span>(OUTPUT_PATH),conf);<br>	 	<span class="hljs-comment">// 若输出目录已存在则删除</span><br>		<span class="hljs-keyword">if</span>(fs.exists(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH)))<br>				fs.delete(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br>		<br>		<span class="hljs-comment">// 开启一个作业</span><br>		<span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Job</span>(conf,<span class="hljs-string">&quot;myjob&quot;</span>);<br>		<span class="hljs-comment">// 设置作业jar包</span><br>		job.setJarByClass(MyWordCount.class);<br>		<span class="hljs-comment">// 设置作业Mapper类</span><br>		job.setMapperClass(MyMapper.class);<br>		<span class="hljs-comment">// 设置作业Reducer类</span><br>		job.setReducerClass(MyReduce.class);<br>		<br>		<span class="hljs-comment">// Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;定义Mapper泛型输出类</span><br>		<br>		<span class="hljs-comment">// Reducer&lt;Text, IntWritable, Text, IntWritable&gt;定义Reducer泛型输出类，因输入与输出相同可省略</span><br>		job.setOutputKeyClass(Text.class);<br>		job.setOutputValueClass(IntWritable.class);<br><br>		<span class="hljs-comment">// 使用文件读取系统读取文件到作业</span><br>		FileInputFormat.addInputPath(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(INPUT_PATH));<br>		<span class="hljs-comment">// 使用文件读取系统输出作业结果</span><br>		FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br>		<br>		job.waitForCompletion(<span class="hljs-literal">true</span>);<br><br>	&#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h4 id="排序案例"><a href="#排序案例" class="headerlink" title="排序案例"></a>排序案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">7 5<br>2 1<br>2 2<br>9 3<br>1 8<br>4 5<br>6 2<br>0 7<br><br>Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;<br>public void map(LongWritable k1, Text v1, Context context)<br>输入LongWritable k1, Text v1(LongWritable, Text)：序号,行<br>处理<br>输出Context context(MyK2, LongWritable)：两个数,后面那个数(与排序无关,为空都可以)<br><br>Reducer&lt;MyK2, LongWritable,LongWritable, LongWritable&gt;<br>public void reduce(MyK2 myk2, Iterable&lt;LongWritable&gt; v2s,Context context)<br>输入MyK2 myk2, Iterable&lt;LongWritable&gt; v2s(MyK2, LongWritable)：两个数，后面那个数(与排序无关,为空都可以)<br>处理<br>输出Context context(LongWritable, LongWritable)：第一个数,第二个数<br><br>0	7<br>1	8<br>2	1<br>2	2<br>4	5<br>6	2<br>7	5<br>9	3<br></code></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> demo;<br><br><span class="hljs-keyword">import</span> java.io.DataInput;<br><span class="hljs-keyword">import</span> java.io.DataOutput;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.net.URI;<br><span class="hljs-keyword">import</span> java.util.Iterator;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.WritableComparable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;   <br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Sort</span> &#123;<br>	<br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyK2</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">WritableComparable</span>&lt;MyK2&gt;&#123;<br>		 <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> myk2;  <br>		 <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> myv2;  <br>		 <span class="hljs-keyword">public</span> <span class="hljs-title function_">MyK2</span><span class="hljs-params">()</span> &#123;  <br>		        <span class="hljs-comment">// TODO Auto-generated constructor stub  </span><br>		 &#125;  <br>		  <br>		 <span class="hljs-keyword">public</span> <span class="hljs-title function_">MyK2</span><span class="hljs-params">(<span class="hljs-type">long</span> myk2, <span class="hljs-type">long</span> myv2)</span> &#123;  <br>		     <span class="hljs-built_in">this</span>.myk2 = myk2;  <br>		     <span class="hljs-built_in">this</span>.myv2 = myv2;  <br>		 &#125;<br><br>		<span class="hljs-meta">@Override</span><br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">readFields</span><span class="hljs-params">(DataInput in)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>			<span class="hljs-built_in">this</span>.myk2=in.readLong();  <br>	        <span class="hljs-built_in">this</span>.myv2=in.readLong();<br>		&#125;<br><br>		<span class="hljs-meta">@Override</span><br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">write</span><span class="hljs-params">(DataOutput out)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>			out.writeLong(myk2);  <br>	        out.writeLong(myv2);<br>		&#125;<br><br>		<span class="hljs-meta">@Override</span><br>		<span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">compareTo</span><span class="hljs-params">(MyK2 my)</span> &#123;<br>			<span class="hljs-type">long</span> temp=<span class="hljs-built_in">this</span>.myk2-my.myk2; <br>	        <span class="hljs-keyword">if</span>(temp!=<span class="hljs-number">0</span>)&#123;<br>	            <span class="hljs-keyword">return</span> (<span class="hljs-type">int</span>) temp; <br>	        &#125;<br>	        <span class="hljs-keyword">return</span> (<span class="hljs-type">int</span>) (<span class="hljs-built_in">this</span>.myv2-my.myv2);<br>		&#125;  <br>	&#125;<br>	<br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyMapper</span>  <span class="hljs-keyword">extends</span>  <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, MyK2, LongWritable&gt;&#123;  <br>		 <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable k1, Text v1, Context context)</span> <br>						 <span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException<br>		 &#123;<br>			String[]  lines= v1.toString().split(<span class="hljs-string">&quot;\\s&quot;</span>);<br>			<span class="hljs-type">MyK2</span> <span class="hljs-variable">myK2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">MyK2</span>(Long.parseLong(lines[<span class="hljs-number">0</span>]), Long.parseLong(lines[<span class="hljs-number">1</span>]));<br>			context.write(myK2, <span class="hljs-keyword">new</span> <span class="hljs-title class_">LongWritable</span>(Long.parseLong(lines[<span class="hljs-number">0</span>])));<br>			System.out.println(<span class="hljs-string">&quot;map......&quot;</span>);<br>		 &#125;<br>		<br>	&#125;<br>	<br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span>  <span class="hljs-title class_">MyReduce</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;MyK2, LongWritable,LongWritable, LongWritable&gt;&#123;<br>		 <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(MyK2 myk2, Iterable&lt;LongWritable&gt; v2s,Context context)</span> <span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException<br>		 &#123;<br>			 context.write(<span class="hljs-keyword">new</span> <span class="hljs-title class_">LongWritable</span>(myk2.myk2), <span class="hljs-keyword">new</span> <span class="hljs-title class_">LongWritable</span>(myk2.myv2));    <br>			 System.out.println(<span class="hljs-string">&quot;reduce......&quot;</span>);<br>		 &#125;<br>		    <br>	&#125;<br><br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String INPUT_PATH=<span class="hljs-string">&quot;hdfs://master:9000/input/num&quot;</span>;<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String OUTPUT_PATH=<span class="hljs-string">&quot;hdfs://master:9000/output/num/&quot;</span>;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;	<br>		<br>		Configuration  conf=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>		FileSystem  fs=FileSystem.get(<span class="hljs-keyword">new</span> <span class="hljs-title class_">URI</span>(OUTPUT_PATH),conf);<br>	 <br>		<span class="hljs-keyword">if</span>(fs.exists(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH)))<br>				fs.delete(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br>		<br>		Job  job=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Job</span>(conf,<span class="hljs-string">&quot;myjob&quot;</span>);<br>		<br>		job.setJarByClass(Sort.class);<br>		job.setMapperClass(MyMapper.class);<br>		job.setReducerClass(MyReduce.class);<br>		<br>		<span class="hljs-comment">// Mapper&lt;LongWritable, Text, MyK2, LongWritable&gt;定义Mapper泛型输出类</span><br>		job.setMapOutputKeyClass(MyK2.class);<br>		job.setMapOutputValueClass(LongWritable.class);<br>		<span class="hljs-comment">// Reducer&lt;MyK2, LongWritable,LongWritable, LongWritable&gt;定义Reducer泛型输出类</span><br>		job.setOutputKeyClass(LongWritable.class);<br>		job.setOutputValueClass(LongWritable.class);<br>		<br>		FileInputFormat.addInputPath(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(INPUT_PATH));<br>		FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br>		<br>		job.waitForCompletion(<span class="hljs-literal">true</span>);<br>		System.out.println(<span class="hljs-string">&quot;end&quot;</span>);<br>	&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>

<h4 id="图案例"><a href="#图案例" class="headerlink" title="图案例"></a>图案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">输入：<br>child	parent <br>Tom	Lucy<br>Tom	Jack<br>Jone	Lucy<br>Jone	Jack<br>Lucy	Mary<br>Lucy	Ben<br>Jack	 Alice<br>Jack	Jesse<br>Terry	Alice<br>Terry	Jesse<br>Philip	Terry<br>Philip	Alma<br>Mark	Terry<br>Mark	Alma<br>需求出输入中的所有的孙子与祖父母<br><br><br>Mapper&lt;LongWritable, Text, Text, Text&gt;<br>public void map(LongWritable k1, Text v1, Context context)<br>输入LongWritable k1, Text v1(LongWritable, Text)：序号,行<br>处理：读取行里的数据split，并以关系形式保存(以Tom	Lucy为例)：<br>Tom,1,Tom,Lucy<br>Tom,2,Lucy,Tom<br>输出Context context(Text, Text)：人名，这个人与其他人的关系(数组)<br><br>Reducer&lt;Text, Text, Text, Text&gt;<br>public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)<br>输入Text key, Iterable&lt;Text&gt; values(Text, Text)：人名，这个人与其他人的关系(数组)<br>处理：从数组中读出关系并将与该人有关的符合条件的人加入临时数组并输出<br>输出Context context(Text, Text)：孙子，祖父母<br><br>Jone	Alice<br>Jone	Jesse<br>Tom	Alice<br>Tom	Jesse<br>Jone	Mary<br>Jone	Ben<br>Tom	Mary<br>Tom	Ben<br>Mark	Alice<br>Mark	Jesse<br>Philip	Alice<br>Philip	Jesse<br><br></code></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> mr;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.net.URI;<br><span class="hljs-keyword">import</span> java.util.ArrayList;<br><span class="hljs-keyword">import</span> java.util.Iterator;<br><span class="hljs-keyword">import</span> java.util.List;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem ;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.NullWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper.Context;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;   <br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyGL</span> &#123;<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyGLMapper</span>  <span class="hljs-keyword">extends</span>  <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, Text, Text&gt;&#123;  <br>		<br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable k1, Text v1, Context context)</span> <br>			<span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException&#123;<br>			 <br>			<span class="hljs-comment">//  1   2  file   tab  ,</span><br>			String[]  lines = v1.toString().split(<span class="hljs-string">&quot;\t&quot;</span>);		<br>			 <br>			<span class="hljs-keyword">if</span>(lines.length != <span class="hljs-number">2</span> || lines[<span class="hljs-number">0</span>].trim().equals(<span class="hljs-string">&quot;child&quot;</span>))<br>				<span class="hljs-keyword">return</span>;   <span class="hljs-comment">//child  parent</span><br>			<br>			<br>			String word1=lines[<span class="hljs-number">0</span>].trim();  <span class="hljs-comment">//  tom</span><br>			String word2=lines[<span class="hljs-number">1</span>].trim();  <span class="hljs-comment">//  lucy</span><br>			<br>			<br>			context.write(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(word1), <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(<span class="hljs-string">&quot;1&quot;</span>+<span class="hljs-string">&quot;,&quot;</span>+word1+<span class="hljs-string">&quot;,&quot;</span>+word2));<br>			context.write(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(word2), <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(<span class="hljs-string">&quot;2&quot;</span>+<span class="hljs-string">&quot;,&quot;</span>+word1+<span class="hljs-string">&quot;,&quot;</span>+word2));<br>		    System.out.println(<span class="hljs-string">&quot;map......&quot;</span>+word1+<span class="hljs-string">&quot;-&quot;</span>+word2);<br>		&#125;<br>		<br>	&#125;<br>	<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span>  <span class="hljs-title class_">MyGLReduce</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, Text, Text, Text&gt;&#123;<br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;Text&gt; values, Context context)</span><br>			<span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException &#123;<br>			<span class="hljs-comment">/*</span><br><span class="hljs-comment">			* lucy   2+tom+lucy</span><br><span class="hljs-comment">			* lucy   1+lucy+mary</span><br><span class="hljs-comment">			* </span><br><span class="hljs-comment">			* 2--&gt;split[1]  tom</span><br><span class="hljs-comment">			* 1--&gt;split[2]  mary</span><br><span class="hljs-comment">			* </span><br><span class="hljs-comment">			* k3=tom  v3=mary</span><br><span class="hljs-comment">			* */</span><br>			List&lt;String&gt;  grandch = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>();<br>			List&lt;String&gt;  grandpa = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>();<br>			 <br>			Iterator&lt;Text&gt;  it=values.iterator();<br>			<span class="hljs-keyword">while</span>(it.hasNext())&#123;<br>				String  lines= it.next().toString();   <span class="hljs-comment">//2+tom+lucy</span><br>				String[] words=lines.split(<span class="hljs-string">&quot;,&quot;</span>);      <span class="hljs-comment">//[&quot;2&quot;,&quot;tom&quot;,&quot;lucy&quot;]</span><br>				<span class="hljs-keyword">if</span>(words[<span class="hljs-number">0</span>].equals(<span class="hljs-string">&quot;1&quot;</span>))&#123;<br>					grandpa.add(words[<span class="hljs-number">2</span>]);<br>				&#125;<br>				<span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(words[<span class="hljs-number">0</span>].equals(<span class="hljs-string">&quot;2&quot;</span>))&#123;<br>					grandch.add(words[<span class="hljs-number">1</span>]);<br>					<br>				&#125;<br>				<span class="hljs-keyword">else</span><br>					<span class="hljs-keyword">return</span>;<br>				<br>				<br>			&#125;<br>			 <br>			<span class="hljs-keyword">for</span>(String ch:grandch)	<br>				<span class="hljs-keyword">for</span>(String pa:grandpa)&#123;<br>		            context.write(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(ch), <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(pa)); <br>		            System.out.println(<span class="hljs-string">&quot;reduce......&quot;</span>+ch+<span class="hljs-string">&quot; - &quot;</span>+pa);<br>				&#125;<br>			 <br>			System.out.println(<span class="hljs-string">&quot;reduce......&quot;</span>);<br>		&#125;<br>		 <br>       <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">cleanup</span><span class="hljs-params">(Context context)</span> <span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException&#123;<br>			 <br>    	   <br>			 <br>			 <br>		&#125;<br>		    <br>	&#125;<br><br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String INPUT_PATH=<span class="hljs-string">&quot;hdfs://master:9000/input/gl.dat&quot;</span>;<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String OUTPUT_PATH=<span class="hljs-string">&quot;hdfs://master:9000/output/c/&quot;</span>;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;	<br>		<br>		Configuration  conf=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>		FileSystem  fs=FileSystem.get(<span class="hljs-keyword">new</span> <span class="hljs-title class_">URI</span>(OUTPUT_PATH),conf);<br>	 <br>		<span class="hljs-keyword">if</span>(fs.exists(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH)))<br>				fs.delete(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br>		<br>		Job  job=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Job</span>(conf,<span class="hljs-string">&quot;myjob&quot;</span>);<br>		<br>		job.setJarByClass(MyGL.class);<br>		job.setMapperClass(MyGLMapper.class);<br>		job.setReducerClass(MyGLReduce.class);<br>		 <br>		 <br>		job.setOutputKeyClass(Text.class);<br>		job.setOutputValueClass(Text.class);<br>		<br>		 <br>		<br>		FileInputFormat.addInputPath(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(INPUT_PATH));<br>		FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br>		<br>		job.waitForCompletion(<span class="hljs-literal">true</span>);<br><br>	&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>

<!--TODO:http://langyu.iteye.com/blog/992916-->
<!--TODO:http://www.cnblogs.com/zhangchaoyang/articles/2648815.html-->

<h4 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h4><p>矩阵乘法公式：</p>
<img src="矩阵乘法公式.png" />

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">矩阵A(4*3)(i*n)<br>1,2,3<br>4,5,0<br>7,8,9<br>10,11,12<br><br>矩阵B(3*2)(n*j)<br>10,15<br>0,2<br>11,9<br><br>根据矩阵乘法的定义：矩阵A的列数=矩阵B的行数，即矩阵A和矩阵B都有相同的n<br>矩阵乘法的结果是产生(i*j)的矩阵C<br><br>矩阵C(4*2)(i*j)<br>43,46<br>40,70<br>169,202<br>232,280<br><br>1*10+2*0+3*11=43<br>1*15+2*2+3*9=46<br><br>计算每个矩阵C中的元素(i,j)都需要矩阵A的(i,r)与矩阵B的(r,j)相乘再加上下一个r取值[1,n]<br>接下来看看进行一个矩阵计算需要哪些信息：<br>因为每次计算r都是从1到n，所以r的值不需要保存进map，<br>需要：计算结果是在C的哪里即(i,j)，A矩阵对应的值，B矩阵对应的值，这个值来自哪个矩阵(A还是B)<br><br>那么如何唯一标识矩阵C的一个元素呢？使用矩阵C的坐标，将C的坐标(i,j)作为key<br>(哪个矩阵,对应的r,矩阵的值)作为value，这样就可以保存进行矩阵计算的全部信息了<br><br>分类讨论：<br>(i,j为计算C的第(i,j)个元素的值，r取值[1,n])<br>对于矩阵A的值：<br>key(i,j) value(a,A的列即r,A[i,r])<br>对于矩阵B的值：<br>key(i,j) value(b,B的列即r,B[r,j])<br><br>分类讨论的计算过程见下图<br></code></pre></td></tr></table></figure>

<img src="矩阵乘法.png" />

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> demo;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.net.URI;<br><span class="hljs-keyword">import</span> java.util.HashMap;<br><span class="hljs-keyword">import</span> java.util.Iterator;<br><span class="hljs-keyword">import</span> java.util.Map;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MatrixProdect</span> &#123;<br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, Text, Text&gt; &#123;<br><br>		<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">rowNum</span> <span class="hljs-operator">=</span> <span class="hljs-number">4</span>;<span class="hljs-comment">// 矩阵A的行数</span><br>		<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">colNum</span> <span class="hljs-operator">=</span> <span class="hljs-number">2</span>;<span class="hljs-comment">// 矩阵B的列数</span><br>		<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">rowIndexA</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; <span class="hljs-comment">// 矩阵A，当前在第几行</span><br>		<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">rowIndexB</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; <span class="hljs-comment">// 矩阵B，当前在第几行</span><br><br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span><br>				<span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException &#123;<br><br>			<span class="hljs-type">FileSplit</span> <span class="hljs-variable">fs</span> <span class="hljs-operator">=</span> (FileSplit) context.getInputSplit();<br>			<span class="hljs-type">String</span> <span class="hljs-variable">fileName</span> <span class="hljs-operator">=</span> fs.getPath().getName();<br><br>			String[] tokens = value.toString().split(<span class="hljs-string">&quot;,&quot;</span>); <span class="hljs-comment">// 读进一行数据</span><br>			<span class="hljs-keyword">if</span> (<span class="hljs-string">&quot;a&quot;</span>.equals(fileName)) &#123; <span class="hljs-comment">// 通过文件名判断是矩阵A还是矩阵B</span><br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">j</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; j &lt;= colNum; j++) &#123;<br>					<span class="hljs-type">Text</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(rowIndexA + <span class="hljs-string">&quot;,&quot;</span> + j);<br>					<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">r</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; r &lt; tokens.length; r++) &#123;<br>						<span class="hljs-type">Text</span> <span class="hljs-variable">v</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(<span class="hljs-string">&quot;a,&quot;</span> + (r + <span class="hljs-number">1</span>) + <span class="hljs-string">&quot;,&quot;</span> + tokens[r]);<br>						System.out.println(<span class="hljs-string">&quot;map......&quot;</span> + fileName + <span class="hljs-string">&quot;(&quot;</span> + k + <span class="hljs-string">&quot;)&quot;</span> + v);<br>						context.write(k, v);<br>					&#125;<br>				&#125;<br>				rowIndexA++;<span class="hljs-comment">// 每执行一次map方法，扫描矩阵的下一行</span><br>			&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-string">&quot;b&quot;</span>.equals(fileName)) &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; i &lt;= rowNum; i++) &#123;<br>					<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">r</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; r &lt; tokens.length; r++) &#123;<br>						<span class="hljs-type">Text</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(i + <span class="hljs-string">&quot;,&quot;</span> + (r + <span class="hljs-number">1</span>));<br>						<span class="hljs-type">Text</span> <span class="hljs-variable">v</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>(<span class="hljs-string">&quot;b,&quot;</span> + rowIndexB + <span class="hljs-string">&quot;,&quot;</span> + tokens[r]);<br>						System.out.println(<span class="hljs-string">&quot;map......&quot;</span> + fileName + <span class="hljs-string">&quot;(&quot;</span> + k + <span class="hljs-string">&quot;)&quot;</span> + v);<br>						context.write(k, v);<br>					&#125;<br>				&#125;<br>				rowIndexB++;<span class="hljs-comment">// 每执行一次map方法，扫描矩阵的下一行</span><br>			&#125;<br>			<br>		&#125;<br><br>	&#125;<br><br>	<span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyReduce</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, Text, Text, IntWritable&gt; &#123;<br>		<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;Text&gt; values, Context context)</span><br>				<span class="hljs-keyword">throws</span> java.io.IOException, java.lang.InterruptedException &#123;<br>			<br>			Map&lt;String, String&gt; mapA = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashMap</span>&lt;String, String&gt;();<br>			Map&lt;String, String&gt; mapB = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashMap</span>&lt;String, String&gt;();<br><br>			<span class="hljs-comment">// 根据矩阵来分类</span><br>			<span class="hljs-keyword">for</span> (Text value : values) &#123;<br>				String[] val = value.toString().split(<span class="hljs-string">&quot;,&quot;</span>);<br>				<span class="hljs-keyword">if</span> (<span class="hljs-string">&quot;a&quot;</span>.equals(val[<span class="hljs-number">0</span>])) &#123;<br>					mapA.put(val[<span class="hljs-number">1</span>], val[<span class="hljs-number">2</span>]);<br>				&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-string">&quot;b&quot;</span>.equals(val[<span class="hljs-number">0</span>])) &#123;<br>					mapB.put(val[<span class="hljs-number">1</span>], val[<span class="hljs-number">2</span>]);<br>				&#125;<br>			&#125;<br><br>			<span class="hljs-type">int</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>			Iterator&lt;String&gt; mKeys = mapA.keySet().iterator();<br>			<span class="hljs-keyword">while</span> (mKeys.hasNext()) &#123; <span class="hljs-comment">// 取相同的r值的数相乘</span><br>				<span class="hljs-type">String</span> <span class="hljs-variable">mkey</span> <span class="hljs-operator">=</span> mKeys.next();<br>				<span class="hljs-keyword">if</span> (mapB.get(mkey) == <span class="hljs-literal">null</span>) &#123;<br>					<span class="hljs-keyword">continue</span>;<br>				&#125;<br>				result += Integer.parseInt(mapA.get(mkey)) * Integer.parseInt(mapB.get(mkey));<br>			&#125;<br>			System.out.println(<span class="hljs-string">&quot;reduce......&quot;</span> + <span class="hljs-string">&quot;(&quot;</span> + key + <span class="hljs-string">&quot;)&quot;</span> + result);<br>			context.write(key, <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(result));<br>		&#125;<br><br>	&#125;<br><br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-type">String</span> <span class="hljs-variable">INPUT_PATH_A</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;hdfs://master:9000/input/a&quot;</span>;<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-type">String</span> <span class="hljs-variable">INPUT_PATH_B</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;hdfs://master:9000/input/b&quot;</span>;<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-type">String</span> <span class="hljs-variable">OUTPUT_PATH</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;hdfs://master:9000/output/matrix/&quot;</span>;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br><br>		<span class="hljs-type">Configuration</span> <span class="hljs-variable">conf</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>		<span class="hljs-type">FileSystem</span> <span class="hljs-variable">fs</span> <span class="hljs-operator">=</span> FileSystem.get(<span class="hljs-keyword">new</span> <span class="hljs-title class_">URI</span>(OUTPUT_PATH), conf);<br><br>		<span class="hljs-keyword">if</span> (fs.exists(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH)))<br>			fs.delete(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br><br>		<span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Job</span>(conf, <span class="hljs-string">&quot;myjob&quot;</span>);<br><br>		job.setJarByClass(MatrixProdect.class);<br>		job.setMapperClass(MyMapper.class);<br>		job.setReducerClass(MyReduce.class);<br><br>		job.setMapOutputKeyClass(Text.class);<br>		job.setMapOutputValueClass(Text.class);<br><br>		job.setOutputKeyClass(Text.class);<br>		job.setOutputValueClass(IntWritable.class);<br><br>		FileInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(INPUT_PATH_A));<br>		FileInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(INPUT_PATH_B));<br>		FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(OUTPUT_PATH));<br><br>		job.waitForCompletion(<span class="hljs-literal">true</span>);<br>		System.out.println(<span class="hljs-string">&quot;end&quot;</span>);<br>	&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>

<p>计算方法与上面一样，只是矩阵的存储结构不一样。省略了值为0的元素，对于较大且稀疏的矩阵所占存储空间较小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">行,列,值<br></code></pre></td></tr></table></figure>

<img src="矩阵乘法2.png" />

<!--
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// <span class="hljs-doctag">TODO:</span></span><br></code></pre></td></tr></table></figure>
<p>–&gt;</p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2022/08/23/JVM/JVM-2_%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/">← Next JVM-2_类加载子系统</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2022/08/17/BigData/Hive/">Hive Prev →</a></div></div></div><div id="comments"><div class="selector"><button class="gitalk-sel"></button></div><div id="gitalk"></div></div></div><div class="bottom-btn"><div><a id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧</a><a id="to-index" href="#toc-div" title="文章目录">≡</a><a id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Sicmatr1x</a></h1><div id="description"><p>INTP/Traveller/Stoicism/Crypto-anarchism/Minimalism/Cosmopolitanism/English learner/Crypto/Java Engineer</p></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="toc-number">1.</span> <span class="toc-text">名词解释</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">Hadoop：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%844%E4%B8%AAV"><span class="toc-number">1.2.</span> <span class="toc-text">大数据的4个V:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce"><span class="toc-number">1.3.</span> <span class="toc-text">MapReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HBase"><span class="toc-number">1.4.</span> <span class="toc-text">HBase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-Hadoop-Distributed-File-System-%EF%BC%9A"><span class="toc-number">1.5.</span> <span class="toc-text">HDFS(Hadoop Distributed File System)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive"><span class="toc-number">1.6.</span> <span class="toc-text">hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%94%E6%9C%BA%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86OLTP-On-line-Transaction-Processing-%E3%80%81%E8%81%94%E6%9C%BA%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86OLAP-On-Line-Analytical-Processing"><span class="toc-number">1.7.</span> <span class="toc-text">联机事务处理OLTP(On-line Transaction Processing)、联机分析处理OLAP(On-Line Analytical Processing)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sqoop"><span class="toc-number">1.8.</span> <span class="toc-text">Sqoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZooKepper"><span class="toc-number">1.9.</span> <span class="toc-text">ZooKepper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mahout"><span class="toc-number">1.10.</span> <span class="toc-text">Mahout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Hadoop"><span class="toc-number">2.</span> <span class="toc-text">安装Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%B9%B3%E5%8F%B0"><span class="toc-number">2.1.</span> <span class="toc-text">支持平台</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%EF%BC%9A"><span class="toc-number">2.2.</span> <span class="toc-text">步骤：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-VMware"><span class="toc-number">2.2.1.</span> <span class="toc-text">安装 VMware</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-Ubuntu"><span class="toc-number">2.2.2.</span> <span class="toc-text">安装 Ubuntu</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-jdk"><span class="toc-number">2.2.3.</span> <span class="toc-text">安装 jdk</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Hadoop-1"><span class="toc-number">2.2.4.</span> <span class="toc-text">安装Hadoop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95hadoop"><span class="toc-number">2.2.5.</span> <span class="toc-text">测试hadoop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8hadoop%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8D%95%E7%8B%AC%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.2.6.</span> <span class="toc-text">使用hadoop的本地单独模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.2.7.</span> <span class="toc-text">克隆虚拟机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81IP"><span class="toc-number">2.2.8.</span> <span class="toc-text">配置静态IP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6"><span class="toc-number">2.2.9.</span> <span class="toc-text">修改hosts文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85ssh"><span class="toc-number">2.2.10.</span> <span class="toc-text">安装ssh</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEHadoop%E9%9B%86%E7%BE%A4"><span class="toc-number">2.2.11.</span> <span class="toc-text">配置Hadoop集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-MapReduce"><span class="toc-number">2.2.12.</span> <span class="toc-text">安装 MapReduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8yarn"><span class="toc-number">2.2.13.</span> <span class="toc-text">启动yarn</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">3.</span> <span class="toc-text">HDFS文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#JVM%E4%BB%8EHDFS%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">JVM从HDFS读取文件流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E7%9A%84%E7%BB%84%E7%BB%87%E4%B8%8E%E8%AF%BB%E5%86%99%EF%BC%9A"><span class="toc-number">3.1.1.</span> <span class="toc-text">HDFS文件存储的组织与读写：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-IO"><span class="toc-number">3.2.</span> <span class="toc-text">Hadoop IO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce-1"><span class="toc-number">4.</span> <span class="toc-text">MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">4.1.</span> <span class="toc-text">结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B"><span class="toc-number">4.2.</span> <span class="toc-text">案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E6%A1%88%E4%BE%8B"><span class="toc-number">4.2.1.</span> <span class="toc-text">单词统计案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B"><span class="toc-number">4.2.2.</span> <span class="toc-text">排序案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E6%A1%88%E4%BE%8B"><span class="toc-number">4.2.3.</span> <span class="toc-text">图案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">4.2.4.</span> <span class="toc-text">矩阵乘法</span></a></li></ol></li></ol></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr>主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/search.js"></script><script class="pjax-js">reset=_=>{gitalk = new Gitalk({
 clientID: '',
 clientSecret: '',
 repo: 'blog-comments',
 owner: 'Sicmatr1x',
 admin: ['Sicmatr1x'],
 distractionFreeMode: false,
 id: 
});
if (document.querySelector("#gitalk")) gitalk.render("gitalk");code.findCode();}</script><script src="/js/arknights.js"></script><script src="/js/pjax.js"></script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script></body></html>