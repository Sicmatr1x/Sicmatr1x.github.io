<!DOCTYPE html><html lang="en" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HDFS | Sicmatr1x's Blog</title><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - 共 $1 行","copy":"复制","copyFinish":"复制成功","expand":"展开"}}</script><script src="/js/gitalk.js"></script><script src="//unpkg.com/mermaid@9.2.2/dist/mermaid.min.js"></script><link rel="stylesheet" href="/css/arknights.css"><script>if (window.localStorage.getItem('theme-mode') === 'light') document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark') document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Sicmatr1x's Blog" type="application/atom+xml">
</head><body><div class="loading" style="opacity: 0"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><nav><div class="navBtn hide"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a class="navBlock" href="/toolkit/"><span class="navItemTitle">Toolkit</span></a></li><li class="navItem"><a class="navBlock" href="/about/"><span class="navItemTitle">About</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>HDFS</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2022-08-13T09:19:43.000Z" id="date"> 2022-08-13</time></div></span><br><span>最后更新时间: <div class="control"><time datetime="2023-03-25T16:00:00.000Z" id="updated"> 2023-03-26</time></div></span></div></div><hr><div id="post-content"><h2 id="HDFS启动与关闭"><a href="#HDFS启动与关闭" class="headerlink" title="HDFS启动与关闭"></a>HDFS启动与关闭</h2><p>HDFS 和普通的硬盘上的文件系统不一样，是通过Java 虚拟机运行在整个集群当中的，<br>所以当Hadoop 程序写好之后，需要启动HDFS 文件系统，才能运行。</p>
<p>HDFS启动过程如下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bin/start-dfs.sh<br></code></pre></td></tr></table></figure>


<p>这一脚本会启动NameNode ， 然后根据conf&#x2F;slaves 中的记录逐个启动DataNode ，最后<br>根据conf&#x2F;masters 中记录的Secondary NameNode 地址启动SecondaryNameNode 。</p>
<p>HDFS 关闭过程如下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bin/stop-dfs.sh<br></code></pre></td></tr></table></figure>


<p>这一脚本的运行过程正好是bin&#x2F;start-dfs.sh 的逆过程，关闭Secondary NameNode ，然后<br>是每个DataNode ，最后是NameNode自身。</p>
<h2 id="HDFS命令基本格式"><a href="#HDFS命令基本格式" class="headerlink" title="HDFS命令基本格式:"></a>HDFS命令基本格式:</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -cmd &lt; args &gt;<br></code></pre></td></tr></table></figure>


<h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a><code>ls</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">ls</span>  /<br></code></pre></td></tr></table></figure>


<p>列出hdfs文件系统根目录下的目录和文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">ls</span> -R /<br></code></pre></td></tr></table></figure>


<p>列出hdfs文件系统所有的目录和文件</p>
<h3 id="put"><a href="#put" class="headerlink" title="put"></a><code>put</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -put &lt; <span class="hljs-built_in">local</span> file &gt; &lt; hdfs file &gt;<br></code></pre></td></tr></table></figure>


<p>hdfs file的父目录一定要存在，否则命令不会执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -put  &lt; <span class="hljs-built_in">local</span> file or <span class="hljs-built_in">dir</span> &gt;...&lt; hdfs <span class="hljs-built_in">dir</span> &gt;<br></code></pre></td></tr></table></figure>


<p>hdfs dir 一定要存在，否则命令不会执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -put - &lt; hdsf  file&gt;<br></code></pre></td></tr></table></figure>


<p>从键盘读取输入到hdfs file中，按Ctrl+D结束输入，hdfs file不能存在，否则命令不会执行</p>
<h3 id="moveFromLocal"><a href="#moveFromLocal" class="headerlink" title="moveFromLocal"></a><code>moveFromLocal</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -moveFromLocal  &lt; <span class="hljs-built_in">local</span> src &gt; ... &lt; hdfs dst &gt;<br></code></pre></td></tr></table></figure>


<p>与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中</p>
<h3 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a><code>copyFromLocal</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -copyFromLocal  &lt; <span class="hljs-built_in">local</span> src &gt; ... &lt; hdfs dst &gt;<br></code></pre></td></tr></table></figure>


<p>与put相类似，也可以从从键盘读取输入到hdfs file中</p>
<h3 id="get"><a href="#get" class="headerlink" title="get"></a><code>get</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -get &lt; hdfs file &gt; &lt; <span class="hljs-built_in">local</span> file or <span class="hljs-built_in">dir</span>&gt;<br></code></pre></td></tr></table></figure>


<p>local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -get &lt; hdfs file or <span class="hljs-built_in">dir</span> &gt; ... &lt; <span class="hljs-built_in">local</span>  <span class="hljs-built_in">dir</span> &gt;<br></code></pre></td></tr></table></figure>


<p>拷贝多个文件或目录到本地时，本地要为文件夹路径</p>
<p><code>注意：</code>如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题，</p>
<h3 id="moveToLocal"><a href="#moveToLocal" class="headerlink" title="moveToLocal"></a><code>moveToLocal</code></h3><p>当前版本中还未实现此命令</p>
<h3 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="copyToLocal"></a><code>copyToLocal</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -copyToLocal &lt; <span class="hljs-built_in">local</span> src &gt; ... &lt; hdfs dst &gt;<br></code></pre></td></tr></table></figure>


<p>与get相类似</p>
<h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a><code>rm</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">rm</span> &lt; hdfs file &gt; ...<br>hadoop fs -<span class="hljs-built_in">rm</span> -r &lt; hdfs <span class="hljs-built_in">dir</span>&gt;...<br></code></pre></td></tr></table></figure>


<p>每次可以删除多个文件或目录</p>
<h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a><code>mkdir</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">mkdir</span> &lt; hdfs path&gt;<br></code></pre></td></tr></table></figure>


<p>只能一级一级的建目录，父目录不存在的话使用这个命令会报错</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">mkdir</span> -p &lt; hdfs path&gt; <br></code></pre></td></tr></table></figure>


<p>所创建的目录如果父目录不存在就创建该父目录</p>
<h3 id="getmerge"><a href="#getmerge" class="headerlink" title="getmerge"></a><code>getmerge</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -getmerge &lt; hdfs <span class="hljs-built_in">dir</span> &gt;  &lt; <span class="hljs-built_in">local</span> file &gt;<br></code></pre></td></tr></table></figure>


<p>将hdfs指定目录下所有文件排序后合并到local指定的<code>文件</code>中，文件不存在时会自动创建，文件存在时会覆盖里面的内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -getmerge -<span class="hljs-built_in">nl</span>  &lt; hdfs <span class="hljs-built_in">dir</span> &gt;  &lt; <span class="hljs-built_in">local</span> file &gt;<br></code></pre></td></tr></table></figure>


<p>加上nl后，合并到local file中的hdfs文件之间会空出一行</p>
<h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a><code>cp</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">cp</span>  &lt; hdfs file &gt;  &lt; hdfs file &gt;<br></code></pre></td></tr></table></figure>


<p>目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在</p>
<p>hadoop fs -cp &lt; hdfs file or dir &gt;… &lt; hdfs dir &gt;</p>
<p>目标文件夹要存在，否则命令不能执行</p>
<h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a><code>mv</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">mv</span> &lt; hdfs file &gt;  &lt; hdfs file &gt;<br></code></pre></td></tr></table></figure>


<p>目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">mv</span>  &lt; hdfs file or <span class="hljs-built_in">dir</span> &gt;...  &lt; hdfs <span class="hljs-built_in">dir</span> &gt;<br></code></pre></td></tr></table></figure>


<p>源路径有多个时，目标路径必须为目录，且必须存在。</p>
<p><code>注意：</code>跨文件系统的移动（local到hdfs或者反过来）都是不允许的</p>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a><code>count</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -count &lt; hdfs path &gt;<br></code></pre></td></tr></table></figure>


<p>统计hdfs对应路径下的目录个数，文件个数，文件总计大小</p>
<p>显示为目录个数，文件个数，文件总计大小，输入路径</p>
<h3 id="du"><a href="#du" class="headerlink" title="du"></a><code>du</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">du</span> &lt; hdsf path&gt; <br></code></pre></td></tr></table></figure>


<p>显示hdfs对应路径下每个文件夹和文件的大小</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">du</span> -s &lt; hdsf path&gt; <br></code></pre></td></tr></table></figure>


<p>显示hdfs对应路径下所有文件和的大小</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">du</span> - h &lt; hdsf path&gt; <br></code></pre></td></tr></table></figure>


<p>显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864</p>
<h3 id="text"><a href="#text" class="headerlink" title="text"></a><code>text</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -text &lt; hdsf file&gt;<br></code></pre></td></tr></table></figure>


<p>将文本文件或某些格式的非文本文件通过文本格式输出</p>
<h3 id="setrep"><a href="#setrep" class="headerlink" title="setrep"></a><code>setrep</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -setrep -R 3 &lt; hdfs path &gt;<br></code></pre></td></tr></table></figure>


<p>改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作</p>
<h3 id="stat"><a href="#stat" class="headerlink" title="stat"></a><code>stat</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hdoop fs -<span class="hljs-built_in">stat</span> [format] &lt; hdfs path &gt;<br></code></pre></td></tr></table></figure>


<p>返回对应路径的状态信息</p>
<p>[format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间）</p>
<p>可以这样书写<code>hadoop fs -stat %b%o%n &lt; hdfs path &gt;</code>，不过不建议，这样每个字符输出的结果不是太容易分清楚</p>
<h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a><code>tail</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">tail</span> &lt; hdfs file &gt;<br></code></pre></td></tr></table></figure>


<p>在标准输出中显示文件末尾的1KB数据</p>
<h3 id="archive"><a href="#archive" class="headerlink" title="archive"></a><code>archive</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop archive -archiveName name.har -p &lt; hdfs parent <span class="hljs-built_in">dir</span> &gt; &lt; src &gt;* &lt; hdfs dst &gt;<br></code></pre></td></tr></table></figure>


<p>命令中参数name：压缩文件名，自己任意取；&lt; hdfs parent dir &gt; ：压缩文件所在的父目录；&lt; src &gt;：要压缩的文件名；&lt; hdfs dst &gt;：压缩文件存放路径</p>
<p>*示例：hadoop archive -archiveName hadoop.har -p &#x2F;user 1.txt 2.txt &#x2F;des</p>
<p>示例中将hdfs中&#x2F;user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中&#x2F;des目录下，如果1.txt，2.txt不写就是将&#x2F;user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中&#x2F;des目录下</p>
<p>显示har的内容可以用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">ls</span> /des/hadoop.jar<br></code></pre></td></tr></table></figure>


<p>显示har压缩的是那些文件可以用如下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">ls</span> -R har:///des/hadoop.har<br></code></pre></td></tr></table></figure>


<p><code>注意：</code>har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。</p>
<h3 id="balancer"><a href="#balancer" class="headerlink" title="balancer"></a><code>balancer</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hdfs balancer<br></code></pre></td></tr></table></figure>


<p>如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程</p>
<h3 id="dfsadmin"><a href="#dfsadmin" class="headerlink" title="dfsadmin"></a><code>dfsadmin</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hdfs dfsadmin -<span class="hljs-built_in">help</span><br></code></pre></td></tr></table></figure>


<p>管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看</p>
<p>hdfs dfsadmin -report</p>
<p>显示文件系统的基本数据</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hdfs dfsadmin -safemode &lt; enter | leave | get | <span class="hljs-built_in">wait</span> &gt;<br></code></pre></td></tr></table></figure>


<p>enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式；</p>
<p>wait：等待离开安全模式</p>
<h3 id="distcp"><a href="#distcp" class="headerlink" title="distcp"></a><code>distcp</code></h3><p>用来在两个HDFS之间拷贝数据</p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2022/08/17/BigData/Hive/">← Next Hive</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2022/07/25/Java/Java-NIO/">Java NIO Prev →</a></div></div></div><div id="comments"><div class="selector"><button class="gitalk-sel"></button></div><div id="gitalk"></div></div></div><div class="bottom-btn"><div><a id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧</a><a id="to-index" href="#toc-div" title="文章目录">≡</a><a id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Sicmatr1x</a></h1><div id="description"><p>INTP/Traveller/Stoicism/Crypto-anarchism/Minimalism/Cosmopolitanism/English learner/Crypto/Java Engineer</p></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%85%B3%E9%97%AD"><span class="toc-number">1.</span> <span class="toc-text">HDFS启动与关闭</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%91%BD%E4%BB%A4%E5%9F%BA%E6%9C%AC%E6%A0%BC%E5%BC%8F"><span class="toc-number">2.</span> <span class="toc-text">HDFS命令基本格式:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ls"><span class="toc-number">2.1.</span> <span class="toc-text">ls</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#put"><span class="toc-number">2.2.</span> <span class="toc-text">put</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#moveFromLocal"><span class="toc-number">2.3.</span> <span class="toc-text">moveFromLocal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#copyFromLocal"><span class="toc-number">2.4.</span> <span class="toc-text">copyFromLocal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#get"><span class="toc-number">2.5.</span> <span class="toc-text">get</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#moveToLocal"><span class="toc-number">2.6.</span> <span class="toc-text">moveToLocal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#copyToLocal"><span class="toc-number">2.7.</span> <span class="toc-text">copyToLocal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rm"><span class="toc-number">2.8.</span> <span class="toc-text">rm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mkdir"><span class="toc-number">2.9.</span> <span class="toc-text">mkdir</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#getmerge"><span class="toc-number">2.10.</span> <span class="toc-text">getmerge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cp"><span class="toc-number">2.11.</span> <span class="toc-text">cp</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mv"><span class="toc-number">2.12.</span> <span class="toc-text">mv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count"><span class="toc-number">2.13.</span> <span class="toc-text">count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#du"><span class="toc-number">2.14.</span> <span class="toc-text">du</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#text"><span class="toc-number">2.15.</span> <span class="toc-text">text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#setrep"><span class="toc-number">2.16.</span> <span class="toc-text">setrep</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stat"><span class="toc-number">2.17.</span> <span class="toc-text">stat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tail"><span class="toc-number">2.18.</span> <span class="toc-text">tail</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#archive"><span class="toc-number">2.19.</span> <span class="toc-text">archive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#balancer"><span class="toc-number">2.20.</span> <span class="toc-text">balancer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dfsadmin"><span class="toc-number">2.21.</span> <span class="toc-text">dfsadmin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distcp"><span class="toc-number">2.22.</span> <span class="toc-text">distcp</span></a></li></ol></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr>主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/search.js"></script><script class="pjax-js">reset=_=>{gitalk = new Gitalk({
 clientID: '',
 clientSecret: '',
 repo: 'blog-comments',
 owner: 'Sicmatr1x',
 admin: ['Sicmatr1x'],
 distractionFreeMode: false,
 id: 
});
if (document.querySelector("#gitalk")) gitalk.render("gitalk");code.findCode();}</script><script src="/js/arknights.js"></script><script src="/js/pjax.js"></script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script></body></html>